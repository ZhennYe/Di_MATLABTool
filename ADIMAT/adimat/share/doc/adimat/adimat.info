This is adimat.info, produced by makeinfo version 4.13 from
info file generated from .//adimat.sgml by means of linuxdoc-tools.




File: adimat.info,  Node: Top,  Next: Introduction,  Up: (dir)

The ADiMat Handbook
*******************

     Johannes Willkomm `willkomm@sc.rwth-aachen.de'
     Andre Vehreschild `vehreschild@sc.rwth-aachen.de'
     Version 0.7, May, 2013

   This Handbook describes ADiMat Version 0.5.9

* Menu:

* Introduction::
* Installation::
* Supported Matlab constructs::
* Using ADiMat::
* Tips and recipes::
* The runtime environment::
* The derivative classes::
* Usage::
* Usage of classic adimat::
* Directives::
* Environment variables::
* Dependecy graphs and further information::
* Outlining - Simplifying complex expressions::
* Builtins::
* ToDo::
* Questions and Answers::
* Examples::
* Known bugs and limitations::
* References::
* Copyright::


File: adimat.info,  Node: Introduction,  Next: Installation,  Prev: Top,  Up: Top

1 Introduction
**************

ADiMat is a tool to differentiate Matlab code using a source
transformation approach implementing the forward or reverse mode of
automatic differentiation. All commonly used language features of
Matlab 6 upto 9 are supported, for a detailed description look at
chapter *note Supported Matlab constructs:: . ADiMat's development is
in progress. New features will be added in the future.

* Menu:

* Changes::


File: adimat.info,  Node: Changes,  Up: Introduction

1.1 Changes
===========

Version 0.1 Initial version.

   Version 0.2 Updated description of derivclass. (Sep 10,2003)

   Version 0.3 Added information about Windows support. (Dec 31, 2003)

   Version 0.4 Added documentation of new flags. (Jan 09, 2004)

   Version 0.5 Added documentation of new features and madderiv-class
(Dec 30, 2005).

   Version 0.6 Documentation of new directives and reverse mode (2009).


File: adimat.info,  Node: Installation,  Next: Supported Matlab constructs,  Prev: Introduction,  Up: Top

2 Installation
**************

* Menu:

* How to obtain ADiMat::
* Requirements::
* Installation on Linux or Unix systems::
* Installation on Windows::
* Building and installation on MacOS::


File: adimat.info,  Node: How to obtain ADiMat,  Next: Requirements,  Up: Installation

2.1 How to obtain ADiMat
========================

For academic, research, private or testing purposes, ADiMat is
available free of charge. Binary versions are released routinely for
Windows and Linux for download on the ADiMat server
(https://adimat.sc.informatik.tu-darmstadt.de/download/). MacOS users
can use the Linux version in combination with the source code package
of adimat-client. Users of other Unix versions like Solaris should be
able to do the same.

   If you want to obtain a commercial license, please contact the
authors (mailto:Johannes Willkomm
<johannes.willkomm@sc.tu-darmstadt.de>).


File: adimat.info,  Node: Requirements,  Next: Installation on Linux or Unix systems,  Prev: How to obtain ADiMat,  Up: Installation

2.2 Requirements
================

ADiMat was successfully used on:

   * Windows

   * Linux

   * MacOS

   * Solaris

   Currently not all features are available in the Windows version. The
mex-compiled derivative class mex_derivclass is not available on
Windows.

   ADiMat version 0.5.6 and higher uses the *OpenSSL* library to
contact the transformation server. The Linux packages of ADiMat are
compiled on Debian stable, thus they should run on most common Linux
distributions. However, there are also statically linked packages, i.e.
all required library code is in the executable. The Windows packages of
ADiMat are statically linked.

   *Matlab* or *GNU Octave* is needed to run the differentiated code.


File: adimat.info,  Node: Installation on Linux or Unix systems,  Next: Installation on Windows,  Prev: Requirements,  Up: Installation

2.3 Installation on Linux or Unix systems
=========================================

  1. Unpack the download archive. A directory called
     `adimat-0.6.0-4726-GNU_Linux-i686' (32 Bit version) or
     `adimat-0.6.0-4726-GNU_Linux-x86_64' (64 Bit version) is created.
     This is the _ADiMat home directory_. Before you proceed with the
     following steps, you may rename or move this directory as you
     like, for example to `/opt/adimat'.

  2. Within Matlab (or Octave), navigate to the directory where you
     installed ADiMat, which contains this file (Install.txt). Execute
     the following command:

          ADiMat_startup

     This adds several directories of ADiMat to the path, and also sets
     the environment variable $ADIMAT_HOME. You probably wish to make
     this change permanent, to do this, execute `ADiMat_startup' from
     your `startup.m' (Matlab) or `~/.octaverc' (Octave). For example
     add this line:

          run('/opt/adimat/ADiMat_startup');


   The following two steps are optional, if you want to also use ADiMat
from the command line:

  1. Create an environment variable, ADIMAT_HOME, that contains the
     path to the ADiMat home directory. To do this, in your shell (sh,
     bash, or zsh) execute this command:

          export ADIMAT_HOME=/opt/adimat

     Add this line to your shell's startup file, e.g. `~/.bashrc', to
     set it at startup time.

  2. You should also set your PATH variable to include $ADIMAT_HOME/bin,
     which you do by typing something like this in your `~/.bashrc'

          export PATH=$PATH:$ADIMAT_HOME/bin

     Optionally, also set

          INFOPATH=$INFOPATH:$ADIMAT_HOME/share/doc/adimat

     to read this documentation in the info pager.


   For any questions feel free to contact us via the ADiMat Users
mailing list (mailto:adimat-users@lists.sc.informatik.tu-darmstadt.de).

   In order to build and install ADiMat from the sources, read the file
BUILD in the base directory of the ADiMat source distribution.


File: adimat.info,  Node: Installation on Windows,  Next: Building and installation on MacOS,  Prev: Installation on Linux or Unix systems,  Up: Installation

2.4 Installation on Windows
===========================

  1. Unpack the download archive. A directory called
     `adimat-0.6.0-4726-mingw32-i686' is created. This is the _ADiMat
     home directory_. Before you proceed with the following steps, you
     are free to rename or move this directory as you like, for example
     to `c:/adimat'.

  2. Within Matlab (or Octave), navigate to the directory where you
     installed ADiMat, which contains this file (Install.txt). Execute
     the following command:

          ADiMat_startup

     This adds several directories of ADiMat to the path, and also sets
     the environment variable $ADIMAT_HOME. You probably wish to make
     this change permanent, to do this, execute `ADiMat_startup' from
     your `startup.m' (Matlab) or `~/.octaverc' (Octave). For example
     add this line:

          run('c:/adimat/ADiMat_startup');


   The following two steps are optional, if you want to also use ADiMat
from the command line:

  1. Create an environment variable, ADIMAT_HOME, that contains the
     path to the ADiMat home directory. In the control panel, add the
     environment variable ADIMAT_HOME, for example with the value
     `c:/adimat'.

  2. Also, you should also set your PATH variable to include
     `%ADIMAT_HOME%\bin'.


   For any questions feel free to contact us via the ADiMat Users
mailing list (mailto:adimat-users@lists.sc.informatik.tu-darmstadt.de).

   In order to build and install ADiMat from the sources, read the file
BUILD in the base directory of the ADiMat source distribution.


File: adimat.info,  Node: Building and installation on MacOS,  Prev: Installation on Windows,  Up: Installation

2.5 Building and installation on MacOS
======================================

Users of MacOS should please proceed to install the Linux version
first. There is only one binary program in the distribution,
adimat-client, which will not work on MacOS. This must be replaced by
compiling the adimat-client package from source on the MacOS system.

  1.  Download the adimat-client source package,
     `adimat-client-0.6.0-4726-src.tar.gz' from the download page
     (http://adimat.sc.informatik.tu-darmstadt.de/download/).

  2.  Unpack the download archive in some temporary working directory.
     A directory called `adimat-client-0.6.0-4726-src' is created.

  3.  Install required software. To compile adimat-client you need the
     following third-party software:

       1. The autoconf and automake tools. See the reference section
          and this blog post
          (http://jsdelfino.blogspot.de/2012/08/autoconf-and-automake-on-mac-os-x.html)

       2. The tool gengetopt. See the gengetopt home page
          (http://www.gnu.org/software/gengetopt/)

       3. The OpenSSL library and headers

       4. The libxml2 library and headers

  4. Execute the bootstrap script, which runs the various autotools.

          ./bootstrap

  5. Run configure, setting the prefix to your ADIMAT_HOME.

          ./configure --prefix=$ADIMAT_HOME

     You can use the options `-with-openssl' and `-with-libxml2' to
     specify the location of the required libraries.

  6. Run make and make install

          make
          make install

   That's it! You should now have a new adimat-client binary in
`$ADIMAT_HOME/bin'. For any questions feel free to contact us via the
ADiMat Users mailing list
(mailto:adimat-users@lists.sc.informatik.tu-darmstadt.de).


File: adimat.info,  Node: Supported Matlab constructs,  Next: Using ADiMat,  Prev: Installation,  Up: Top

3 Supported Matlab constructs
*****************************

Matlab is a language for technical-mathematical computing. The language
has a huge number of syntactic constructs. Some are known from other
languages like C, Fortran and so on, some are Matlab-specific. Due to
the huge number of syntactic constructs implemented in Matlab and their
sometimes slightly different meaning in a specific context, not all
language features are completely implemented in ADiMat. That is, the
syntactic constructs are parsed, but their meaning is only partially or
not at all known to ADiMat.  A warning message is printed, if ADiMat
encounters an unknown construct, but the differentiation process
continues. If a warning about unknown language features is printed, the
differentiated code has to be checked. Below is a list of the most
commonly used Matlab features that are supported, partially supported
or not supported at all.  The lists below is subject to change as
development of ADiMat continues. These lists are set up at October 2011.

   If you encounter a language construct or a builtin function that is
not treated correctly by ADiMat or not implemented at all, please tell
the author about it. This will help us to enhance and maintain ADiMat.
Especially Matlab builtin functions may be missing their derivative
information in our database.

* Menu:

* Supported Matlab features::
* Partially supported Matlab features::
* Matlab features not supported at all::
* Limitations of the reverse mode (admDiffRev) (in addition to the above)::
* Limitations of the classless vector mode (admDiffVFor)::
* Limitations of admDiffFor and admDiffRev::


File: adimat.info,  Node: Supported Matlab features,  Next: Partially supported Matlab features,  Up: Supported Matlab constructs

3.1 Supported Matlab features
=============================

These syntactic constructs are supported completely. Their
differentiation is tested. If you encounter any problems, tell the
author immediately.

   *  all mathematical operators like (+, -, *, /, \ ...)

   *  real and complex values

   *  vectors, matrices, cell arrays, structures and dynamic structures

   *  all flow control structures

   *  multiple functions per project

   *  local functions and nested functions

   *  global and persistent variables

   *  indexing into matrices and cellarrays

   *  structures

   *  call of nargin/nargout-functions


File: adimat.info,  Node: Partially supported Matlab features,  Next: Matlab features not supported at all,  Prev: Supported Matlab features,  Up: Supported Matlab constructs

3.2 Partially supported Matlab features
=======================================

Some features of Matlab are parsed by ADiMat, but they may not be
treated correctly. That is, the differentiated code may be incorrect.
Whenever such a feature is found, ADiMat issues a warning and continues
its task. The features which are not completely implemented are listed
below:

   *  java function calls (will be treated as structures)

   *  load- and save-command

   *  call of shell-commands (!-operator)

   *  varargin- and varargout-constructs

   *  function handles, if they are declared via the *note adimat
     Directives::  directive


File: adimat.info,  Node: Matlab features not supported at all,  Next: Limitations of the reverse mode (admDiffRev) (in addition to the above),  Prev: Partially supported Matlab features,  Up: Supported Matlab constructs

3.3 Matlab features not supported at all
========================================

Some Matlab features are not supported at all. These are listed below:

   *  classes

   *  private functions (functions that reside in a private directory
     of the current path).

   *  Matlab extension, or MEX functions, written in C/C++ or Fortran.
     If you have MEX-functions in your project, contact the author.
     Research was done to support these, too.


File: adimat.info,  Node: Limitations of the reverse mode (admDiffRev) (in addition to the above),  Next: Limitations of the classless vector mode (admDiffVFor),  Prev: Matlab features not supported at all,  Up: Supported Matlab constructs

3.4 Limitations of the reverse mode (admDiffRev) (in addition to the above)
===========================================================================

   * On-the-fly creation or enlargement of array and structs is
     supported. For example x(4) = 17, where x did not exist before.
     However creation or enlargement is only allowed on the last
     subscripting level. For example, in x(4).abc.def = 17 the
     subexpression x(4).abc must be valid to read before the assignment,
     and in x(4).abc.def(2) = 17 the subexpression x(4).abc.def must be
     valid to read.

   * Enlargement of arrays is only supported if all the fields assigned
     to are created. E.g. x(4) = 17; x(2:6) = 12; is not allowed as the
     second assignment assigns to 2:4, which already exists, but also
     creates 5:6.

   * Variables first assigned to in a loop pose a certain problem.
     ADiMat inserts initialiation with 0 at the beginning of the
     function. This fails if the first assignment is of more than one
     dimension. ADiMat may be able to detect this and create better
     initializations in the future. If the variable is recognized to
     have type struct, then the initialization is with struct instead
     of 0. This may create new problems as the next assignment may fail
     as the two structs have different components, while it does not
     fail when the variable is still unassigned. As a workaround,
     initialize your variables at the top of your functions
     (unconditional initialization).

   * return is not supported, more precisely, it is currently ignored
     when inside a branch. An early return on the main statement level
     is allowed. Hence, there is no problem when the return is inside a
     branch and is not actually run through, as in the following
     example:

          if mistake
            fprintf(2, 'error: a mistake occured')
            return
          end
          more code
          return
          unused code

     This should work if the mistake does not happen. The second return
     is used to basically comment out the unused code, and this is
     supported.

   * try and catch are not supported

   * Missing builtin functions declared with the BMFUNC directive are
     not differentiated, the RM has its own extension mechanism

   * Numeric indices must not have repeated entries. Both `x = y([1 1 2
     2])' and `x([1 1 2 2]) = y' are nto supported.


File: adimat.info,  Node: Limitations of the classless vector mode (admDiffVFor),  Next: Limitations of admDiffFor and admDiffRev,  Prev: Limitations of the reverse mode (admDiffRev) (in addition to the above),  Up: Supported Matlab constructs

3.5 Limitations of the classless vector mode (admDiffVFor)
==========================================================

The limitations listed here are in addition to the limitations of
ADiMat in general.

   * Index operations are by default wrapped in calls to the runtime
     functions `adimat_opdiff_subsref' and `adimat_opdiff_subsasgn'.
     These are rather slow. However, your code may not actually need
     these runtime functions. They can be avoided if you set the
     following transformation options:

          opts.paramaters.useSubsref = 0
          opts.paramaters.useSubsasgn = 0

     If you do this, the indexing into matrices in your code should be
     "correct". That means, that the indices used should reflect the
     shape of the resulting value:

        * components of a row vector s should always be indexed into
          with either s(1,ind) or s(:,ind)

   * Missing builtin functions declared with the BMFUNC directive are
     not differentiated


File: adimat.info,  Node: Limitations of admDiffFor and admDiffRev,  Prev: Limitations of the classless vector mode (admDiffVFor),  Up: Supported Matlab constructs

3.6 Limitations of admDiffFor and admDiffRev
============================================

The *note High level user interface::  functions admDiffFor and
admDiffRev have some limitations, which are listed here. In some cases
there are workarounds, but in some other cases you may have to use the
lower level interface, using the recipes explained in other sections of
this manual.

   * Function handles are partially supported by ADiMat using the *note
     adimat Directives::  directive, but support for this has not yet
     been added to the high-level interface.


File: adimat.info,  Node: Using ADiMat,  Next: Tips and recipes,  Prev: Supported Matlab constructs,  Up: Top

4 Using ADiMat
**************

It is assumed, that the reader is familiar with the general technique
of automatic differentiation (AD). That is, the keywords independent and
dependent variable and top-level function are familiar to him. Some
recommended literature on the subject is the book *note References::
or the papers *note References::  or *note References:: .

* Menu:

* General description::
* High level user interface::
* Higher order derivatives::
* Source code transformation::
* Running the transformed code::
* Applying AD to an example::
* Using sparsity exploitation or compression::


File: adimat.info,  Node: General description,  Next: High level user interface,  Up: Using ADiMat

4.1 General description
=======================

ADiMat applies the source transformation approach of automatic
differentiation to Matlab programs. A Matlab program consists of one or
more m-files defining functions. The file that contains the starting
point of the computation (the top level function) is called the master
file. The computation of derivatives with ADiMat consists of two steps:

  1. *Transformation* of the source code to produce differentiated code.

  2. *Evaluation* of the transformed function to compute the actual
     derivative numbers.

   For most users the best option should be to use the functions
admDiffFor, admDiffVFor, and admDiffRev, described in the *note High
level user interface:: , which perform both steps automatically.
However, both steps can be still done individually, which may be
necessary when you want to use special features of ADiMat. If you are
interested in the details, read the sections *note Source code
transformation::  and *note Running the transformed code::  later in
this chapter.


File: adimat.info,  Node: High level user interface,  Next: Higher order derivatives,  Prev: General description,  Up: Using ADiMat

4.2 High level user interface
=============================

The Jacobian matrix of the top-level function, evaluated at certain
arguments, is computed by the functions admDiffFor, admDiffVFor, and
admDiffRev. These functions all have the same interface, and the expect
the name of or a handle to the top level function, the seed matrix and
the function arguments in that order. Thus you can easily switch
between forward and reverse mode and experiment with different seed
matrices.

   * In _forward mode_ (FM) of AD, you can compute the product J*S of
     the _Jacobian matrix_ J of a function f at arguments arg1, ...,
     argN and a _seed matrix_ S. In theory, the computational expense
     (time and memory) depends linearly on the number of *columns* of S.

     The following functions implement the forward mode of AD in ADiMat:

    `admDiffFor(@f, S, arg1, ..., argN, opts)'
          Classic FM implementation in ADiMat.

    `admDiffVFor(@f, S, arg1, ..., argN, opts)'
          New FM implementation in ADiMat.


   * In _reverse mode_ (RM) of AD, you can compute the product S*J of a
     seed matrix S and the Jacobian matrix J of a function f at
     arguments arg1, ..., argN. In theory, the computational expense in
     time depends linearly on the number of *rows* of S. The memory
     required is in the order of the runtime of f, as all variable
     values that are overwritten have to be recorded on one of the
     *note Stacks for the reverse mode:: . You can use the directive
     *note admproc Directives::  to trade off memory for runtime.

    `admDiffRev(@f, S, arg1, ..., argN, opts)'
          RM implementation in ADiMat.



   Options can be given to the *admDiff** routines using a structure
constructed with the function *admOptions*. It must be given to them as
the last argument. The function *admOptions* takes a sequence of name
and value pairs, as shown in the following example:

     opts = admOptions('independents', [1]);

   Passing this options struct to *admDiff** tells ADiMat to
differentiate the function only with respect to the first parameter.
Some names can also be abbreviated, e.g. 'i' can be used as short for
'independendents', and 'd' can be used for 'dependendents':

     opts = admOptions('i', [1]);

   For the full list of abbreviations refer to the online help of
admOptions. You can also set items of the option struct by assigning to
it:

     opts.independents = [1];

   Derivatives can also be computed with ADiMat using one of two
_numerical differentiation_ methods. Thus you can easily check AD
results by comparing them to the results of these two non-AD methods.
Both methods descibed below are conceptually comparable to the forward
mode of AD, i.e. they return a product J*S of the Jacobian of f and the
given seed matrix S.

   * The _finite differences_ (FD) method, sometimes also called the
     divided differences method, is almost always applicable, except
     near discontinuities, but it produces only quite inaccurate
     derivatives, cf.  Numerical differentiation. You should use the
     option fdStep to set the step length h to the adequate value for
     your problem. The central FD method is used by default, you can
     also specify the forward or backward FD method via the option
     fdMode.

    `admDiffFD(@f, S, arg1, ..., argN, opts)'
          Implementation of the finite differences method in ADiMat.


   * The _complex variable_ method was first divised by Lyness and
     Moler, and provides accurate derivatives, but it is only
     applicable then the function f is _real analytic_, cf. Complex
     variable methods.

    `admDiffComplex(@f, S, arg1, ..., argN, opts)'
          Implementation of the complex variable method in ADiMat.


   With these two functions, you may also differentiate anonymous or
lambda functions, as the function source code is not required. For
example:

     J = admDiffFD(@(x,y) atan2(x, y), 1, 1, 2)


File: adimat.info,  Node: Higher order derivatives,  Next: Source code transformation,  Prev: High level user interface,  Up: Using ADiMat

4.3 Higher order derivatives
============================

ADiMat can, to a limited extend, compute higher order derivatives as
well. Some ADiMat interface compute univariate Taylor coefficients, and
there is also a driver to compute Hessian matrices. In this section we
consider the function polynom:

     function r = polynom(x, c)

* Menu:

* Taylor-Coefficients::
* Hessians::
* Taylor-Coefficients in Reverse Mode::


File: adimat.info,  Node: Taylor-Coefficients,  Next: Hessians,  Up: Higher order derivatives

4.3.1 Taylor-Coefficients
-------------------------

The function admTaylorFor can compute exact Taylor coefficients. By
setting option derivOrder, you can specify which Taylor coefficients to
compute. Note that all Taylor coefficients up to the maximum derivOrder
will be computed, but only those requested will be returned.
admTaylorFor uses an operator overloading (OO) approach, so there is no
source transformation involved.

     Taylor1_10 = admTaylorFor(@polynom, 1, 2, [1 1 1 1 1], admOptions('o', 1:10))

   The Taylor objects that admTaylorFor uses can also be used manually,
for more information see the example *note Examples:: .  Another
version of the function is admTaylorVFor, which uses a similar AD
approach as admDiffVFor:

     Taylor1_10 = admTaylorVFor(@polynom, 1, 2, [1 1 1 1 1], admOptions('o', 1:10))

   admDiffFD and admDiffFor can also compute Taylor coefficients.
admDiffFD can compute up to fourth order Taylor coefficients. Note that
admDiffFD will only compute the derivative orders requested. admDiffFor
can compute second order Taylor coefficients. Note that this is
implemented by computing the full Hessian matrix and then returning
only the diagonal.


File: adimat.info,  Node: Hessians,  Next: Taylor-Coefficients in Reverse Mode,  Prev: Taylor-Coefficients,  Up: Higher order derivatives

4.3.2 Hessians
--------------

* Menu:

* Hessians using admHessian::
* Shortcut Hessian drivers::
* Hessians by recursive application of admDiff::


File: adimat.info,  Node: Hessians using admHessian,  Next: Shortcut Hessian drivers,  Up: Hessians

4.3.2.1 Hessians using admHessian
.................................

admDiffHessian is the general driver for computing a Hessian matrix.
You can use it as follows:

     adopts = admOptions;
     H = admHessian(@polynom, 1, x, c, adopts)
     H = admHessian(@polynom, W, x, c, adopts)
     H = admHessian(@polynom, {Y, W}, x, c, adopts)
     H = admHessian(@polynom, {'s', W}, x, c, adopts)
     H = admHessian(@polynom, {Y, V, W}, x, c, adopts)
     H = admHessian(@polynom, {'s', V, W}, x, c, adopts)

   It can use one of two basic strategies, which are selected by the
option hessianStrategy in the options structure that may be given as
the last argument:

   * 't1rev': Forward over reverse mode

   * 't2for': Interpolation of second order Taylor coefficients

   * 'for2': Second order forward mode

   The first is the default. It uses the same OO approach as
admTaylorFor, but it runs the function differentiated in RM, not the
original function. This approach can compute the full Hessian with an
overhead of O(n).

   The second strategy, 't2for' relies on one of functions that can
compute second order Taylor-coefficients. The option admDiffFunction
controls which underlying function is used to compute the required
univariate Taylor coefficients. Possible options are admTaylorFor,
admTaylorVFor, and admDiffFD.

   The third strategy, 'for2' invokes the function admDiffFor2 which
computes second order derivatives by running a code produced by
applying the FM ST twice. This mode can be either used with the
'scalar_directderivs' RE, which results in strip-mining in scalar mode,
or you can use one of the derivative classes 'opt_derivclass' or
'opt_sp_derivclass', which will use vector mode.

   The second argument specifies the seed matrices. It may either be a
single matrix W, or a cell array of two matrices {Y, W}, or a cell
array of three matrices {Y, V, W}. As usual, a scalar 1 is a shortcut
for a conforming identity matrix. Matrices that are not specified are
also treated as if a scalar 1 was given.

   If we first consider a scalar function, and let Y = 1, then
admHessian will return the matrix product $V^(T) * H * W$. That is, V
must be a (n x p) matrix and W must be a (n x q) matrix. With strategy
t1rev, such a Hessian-matrix product has an overhead of O(q), and with
strategy t2for or for2 it has an overhead of O(p*q). Strategy t2for has
an upper limit of costs as O(n*(n+1)/2), as these are the costs for
computing a full Hessian.

   When the function is vector valued, the result will be the product
$V^(T) * H * W$ of the `component Hessians' H of the m output
components arranged in a (p x q x m) tensor array. With strategy t1rev,
the overhead is then O(q*m), and with strategy t2for or for2 the
overhead is O(p*q).

   The second argument may also be a cell array of three matrices {Y,
V, W}. In this case, V and W are as above and Y must be a (r x m)
matrix, the so called adjoint weight matrix. The result will then be a
tensor of dimensions (p x q x r) with the r weighted sums of the
component Hessians. In particular, if the adjoint weight matrix is a
row vector of length m, then admHessian will compute a weighted sum of
the Hessians. With strategy t1rev, the overhead is then O(q*r), and
with strategy t2for or for2 the overhead is O(p*q).

   A special value for Y is the string 's', or 'sum', which expands to
a row vector of ones, that is, admHessian will then compute the sum of
Hessians. An alternative way to set a adjoint weight matrix Y is to use
the option field seedRev.

   Setting a adjoint weight matrix is particularly interesting when the
sum of component Hessians weighed by Lagrangian multipliers is desired,
such as it is required by the fmincon solver. In this case, it is not
necessary to construct a function for multiplying the contraints by the
Lagrangian multiplier. Just set the adjoint weight matrix to the
Lagrangian weights and differentiate the original constraints function.
This has the advantage that you obtain both the Jacobian of the
original function and the Hessian of the Lagrangian in one go and you
can exploint the efficiency of the reverse mode.


File: adimat.info,  Node: Shortcut Hessian drivers,  Next: Hessians by recursive application of admDiff,  Prev: Hessians using admHessian,  Up: Hessians

4.3.2.2 Shortcut Hessian drivers
................................

While admHessian defaults to the FM-over-RM mode, you have to set some
options to get the other modes described in the previous section.
Therefor we provide some convenience drivers, for the case of
hessianStrategy set to t2for, as follows:

   * admHessFor: admDiffFunction set to admTaylorFor

   * admHessVFor: admDiffFunction set to admTaylorVFor

   * admHessFD: admDiffFunction set to admDiffFD

   For hessianStrategy set to for2, you can also invoke admDiffFor2
directly.

   Finally admHessRev is the driver for Hessians in forward-oer-reverse
mode, which is the default strategy of admHessian.


File: adimat.info,  Node: Hessians by recursive application of admDiff,  Prev: Shortcut Hessian drivers,  Up: Hessians

4.3.2.3 Hessians by recursive application of admDiff
....................................................

You can use the admDiff* functions recursively. Recall the first order
invocation, evaluating first order derivatives of the polynom with
coefficients `c = [1 1 1 1 1]' at `x = 2':

     Jacobian = admDiffFD(@polynom, 1, 2, [1 1 1 1 1])

   You have to tell the outer invocation to differentiate the inner
call only with the respect the function arguments. Thus, the recursive
application for second order derivatives, using admDiffComplex as the
outer differentiation function would be as follows:

     Hessian = admDiffComplex(@admDiffFD, 1, @polynom, 1, 2, [1 1 1 1 1], admOptions('i', [3 4]))

   If you also want to have the Jacobian and the function results, tell
the outer admDiff* function how many outputs to expect from the inner,
but also to only consider the first for differentiation:

     [Hessian, Jacobian, r] = admDiffComplex(@admDiffFD, 1, @polynom, 1, 2, [1 1 1 1 1],
                                    admOptions('i', [3 4], 'nargout', 3, 'd', 1))

   The following combinations of admDiff* functions are possible:

   * FD over FD, Complex, For, VFor, or Rev

   * Complex over FD, For, VFor, or Rev

   You cannot use one of the AD-functions on top of each other, as
ADiMat can usually not differentiate the code it produces. Complex over
Complex does not work as the function to differentiate has to be real
analytic.

   When using FD over FD, you usually have to increase the step length,
to approximately `h = sqrt(sqrt(eps))', when the function arguments are
about one:

     admDiffFD(@admDiffFD, 1, @polynom, 1, 2, [1 1 1 1 1], admOptions('i', [3 4], 'fdStep', sqrt(sqrt(eps))))

   You can compute products of the Hessian times a matrix or vector.
You can pass the latter to either invocation:

     Hv = admDiffFD(@admDiffFD, v, @polynom, 1, 2, [1 1 1 1 1], admOptions('i', [3 4], 'fdStep', 0.001))
     Hv = admDiffFD(@admDiffFD, 1, @polynom, v, 2, [1 1 1 1 1], admOptions('i', [3 4], 'fdStep', 0.001))

   If you use AD as the inner differentiation function, use the first
option, as then the AD process will be run just once.


File: adimat.info,  Node: Taylor-Coefficients in Reverse Mode,  Prev: Hessians,  Up: Higher order derivatives

4.3.3 Taylor-Coefficients in Reverse Mode
-----------------------------------------

The function admTaylorRev can propagate Taylor coefficients across the
function differentiated in RM. This is the same as computing a Hessian
with hessianStrategy set to t1rev, only that option derivOrder is set
to something larger than 1. admTaylorRev uses an operator overloading
(OO) approach for the Taylor coefficients, just like admTaylorFor.

     dTaylor1_10 = admTaylorRev(@polynom, 1, 2, [1 1 1 1 1], admOptions('o', 1:10))

   The same kind of derivatives can be obtained with ADOL-C or CppAD by
performing FM passes up to order M = max(derivOrder) followed by a
single RM pass over the tape.

   The second argument can be a single seed matrix S, or a cell array
of the two seed matrices Y and S. S represents the first order
derivatives of the inputs and Y the first order adjoints of the
outputs. When S is an (n x q) matrix and Y a (p x m) matrix, then the
ouput will be a (n x q x p x M) tensor. As usual, a scalar one stands
for a conforming identity matrix. In particular, setting the second
argument to 1 or to {1, 1}, the result will be a (n x q x m x M) tensor.

   With derivOrder = 1 admTaylorRev returns the same results as
admHessian. In particular, setting derivOrder = 1, and Y = ones(1, m)
will yield the sum of the Hessian matrices of the outputs.


File: adimat.info,  Node: Source code transformation,  Next: Running the transformed code,  Prev: Higher order derivatives,  Up: Using ADiMat

4.4 Source code transformation
==============================

The source transformation step is usually performed implicitly by one
of the *note High level user interface::  functions. They will check
whether the required differentiated function exists and is up-to-date.
Otherwise they use the function admTransform to produce the
differentiated code.

   The following sections explains how to perform this step manually.

* Menu:

* Source transformation from within Matlab::
* Source transformation from the command line::


File: adimat.info,  Node: Source transformation from within Matlab,  Next: Source transformation from the command line,  Up: Source code transformation

4.4.1 Source transformation from within Matlab
----------------------------------------------

From within Matlab, differentiated code can be produced by the function
admTransform. For forward mode transformation is done using the command:

     admTransform(@lighthouse)

   Otherwise, you have to specify the desired transformation mode using
the options structure. For the alternative forward mode transformation
use:

     admTransform(@lighthouse, admOptions('m', 'f'))

   For the reverse mode transformation use:

     admTransform(@lighthouse, admOptions('m', 'r'))


File: adimat.info,  Node: Source transformation from the command line,  Prev: Source transformation from within Matlab,  Up: Source code transformation

4.4.2 Source transformation from the command line
-------------------------------------------------

The transformation can also be done from the command line, using the
command adimat-client (or admproc or admproc-bin, if present). This
requires that the environment variable ADIMAT_HOME is set, see the
*note Installation::  section. The filename of the master file is given
as the command line argument:

     adimat-client lighthouse.m

   This produces code differentiated in forward mode, which is written
to the file g_lighthouse.m. The adimat command currently produces many
errors and warning messages which can for the most part be ignored. If
the result file is generated and the command exits with success
everything should be fine.

   An alternative forward mode transformation can be done using the -f
switch, which produces the file d_lighthouse.m..

     adimat-client -f lighthouse.m

   Reverse mode differentiation of code can be done using the -r
switch, which produces the file a_lighthouse.m.

     adimat-client -r lighthouse.m


File: adimat.info,  Node: Running the transformed code,  Next: Applying AD to an example,  Prev: Source code transformation,  Up: Using ADiMat

4.5 Running the transformed code
================================

As a second step, the differentiated code must be run to compute the
derivative for some input arguments. This should also preferably be
done using the *note High level user interface::

   functions. In the following section describe the steps to perform
this process manually. You can also read this section to gain a better
understanding into the inner workings of the high level interface
functions.

   In this section we consider the function *f*:

     function [y z] = f(a, b)

* Menu:

* Selecting a runtime environment::
* Running forward mode functions::
* Running reverse mode functions::


File: adimat.info,  Node: Selecting a runtime environment,  Next: Running forward mode functions,  Up: Running the transformed code

4.5.1 Selecting a runtime environment
-------------------------------------

In order to run differentiated code, you must first select a suitable
runtime environment. This is done using the function adimat_derivclass,
for example like this:

     adimat_derivclass('opt_derivclass')

   The following combinations of differentiated functions and runtime
environments are possible:

   * Functions differentiated in forward mode (those with prefix *g_*):
     arrderivclass, arrderivclassvxdd, opt_derivclass,
     opt_sp_derivclass, mat_derivclass, or scalar_directderivs.

   * Functions differentiated in alternative forward mode (those with
     prefix *d_*): vector_directderivs.

   * Functions differentiated in reverse mode (those with prefix *a_*):
     arrderivclass, arrderivclassvxdd, opt_derivclass,
     opt_sp_derivclass, mat_derivclass, or scalar_directderivs.

   Using any of the runtime environments, except scalar_directderivs,
enables you to use _vector mode_, i.e. the differentiated function is
run just once, computing derivatives along all derivative directions,
which are given by the columns of the seed matrix, at once.

   Using the scalar_directderivs runtime environment will use _scalar
mode_, i.e. only a single directional derivative can be computed with
each invocation of the differentiated function. The *note High level
user interface::  functions will automatically revert to _strip
mining_, i.e. run the differentiated function once for each derivative
direction specified. If you do the derivative evaluation step manually
you have to take care of this yourself.

   Using one of the runtime environments opt_derivclass,
opt_sp_derivclass, or mat_derivclass, the derivative variables will be
objects of the _derivative class_ contained in the respective runtime
environment. Using these classes tends to be rather slow because of the
overloaded operator resolution at runtime. However, amortization starts
when a large number of directional derivatives are compted.

   Using the runtime environments scalar_directderivs or
vector_directderivs, the derivative variables will be of native arrays
of type double.

   In Octave since version 3.6 classes are supported, and hence also
the ADiMat derivative classes can be used. In older versions only the
runtime environments scalar_directderivs and vector_directderivs are
available.


File: adimat.info,  Node: Running forward mode functions,  Next: Running reverse mode functions,  Prev: Selecting a runtime environment,  Up: Running the transformed code

4.5.2 Running forward mode functions
------------------------------------

To run one of the functions differentiated in forward mode, you have to
perform the following steps:

   * Create derivative input arguments from the function arguments

   * Run the differentiated function

   * Extract derivatives from the derivative output arguments

   For the first step, you should use one of the functions
createFullGradients or createSeededGradientsFor. If you want to compute
the full Jacobian, i.e. use the conforming identity matrix as the seed
matrix, you can use createFullGradients:

     [g_a g_b] = createFullGradients(a, b);

   If you want to specify a seed matrix S, use:

     [g_a g_b] = createSeededGradientsFor(S, a, b);

   For the second step, running the differentiated function, place the
derivative arguments created in the previous step before the respective
arguments in the input argument list, and do likewise for derivative
outputs:

     [g_y y g_z z] = g_f(g_a, a, g_b, b);

   If in doubt, consult the function signature in the file g_f.m or
d_f.m.

   Finally you can extract the computed derivatives from the derivative
outputs. It is recommended to do this using the function admJacFor:

     Jacobian = admJacFor(g_y, g_z);

   You can also manually extract the derivative values from the
derivative outputs. If you are using one of the derivative classes, use
curly brace indexing. To extract the i-th directional derivative of y
from g_y, use the following:

     dirder = g_y{i};

   You can also extract all derivative values at once using the :
wildcard index. This will concatenate all directional derivatives into
one large array. However, the ordering of the derivative values may not
be as you expected and it may differ depending on the derivative class
used.

     dvals = [g_y{:}];

   When you used the alternative forward mode and the
vector_directderivs runtime environment, the i-th directional
derivative is contained in the i-th slice along the first dimension of
the array. The expression d_y(i, :) will always return all those values
in d_y in the form of a row vector, regardless of the number of
dimensions of d_y. You can then use reshape to make the directional
derivative have the same shape as y:

     dirder = reshape(d_y(i, :), size(y));


File: adimat.info,  Node: Running reverse mode functions,  Prev: Running forward mode functions,  Up: Running the transformed code

4.5.3 Running reverse mode functions
------------------------------------

To run a function differentiated in reverse mode, you have to perform
the following steps:

   * Create derivative input arguments from the function results

   * Run the differentiated function

   * Extract derivatives from the derivative output arguments

   In the first step you create _adjoint_ variables of the function's
results. These will be given as _input_ arguments to the adjoint
(reverse mode differentiated) function.

   Thus, for the first step, you need the results that the function
produces at the desired input arguments. Actually only the size and
shape of the results is required, but for simplicity just run the
original function once:

     [y z] = f(a, b);

   Using one of the *note High level user interface::

   functions, you should pass the function results via the option field
functionResults, otherwise admDiffRev will automatically run the
function to obtain this information.

   Now you can use either createFullGradients or
createSeededGradientsRev to construct the adjoint arguments. If you
want to compute the full Jacobian, i.e. use the conforming identity
matrix as the seed matrix, you can use createFullGradients:

     [a_y a_z] = createFullGradients(y, z);

   If you want to specify a seed matrix S, use:

     [a_y a_z] = createSeededGradientsRev(S, y, z);

   For the second step, running the differentiated function, place the
derivative arguments created in the previous step at the end of the
orignal function's argument list. The adjoint outputs come first in the
adjoint function's output list, followed by the function results. Thus,
run the adjoint function like this:

     [a_a a_b y z] = a_f(a, b, a_y, a_z);

   If in doubt, consult the function signature in the file a_f.m.

   The derivate output arguments a_a and a_b are the adjoints of the
function's _input_ parameters, and thus contain the desired derivative
values. You can extract the computed derivatives from them using the
function admJacRev:

     Jacobian = admJacRev(a_a, a_b);

   If you want to extract the derivatives manually, refer to the
description in the previous secion: *note Running forward mode
functions:: .


File: adimat.info,  Node: Applying AD to an example,  Next: Using sparsity exploitation or compression,  Prev: Running the transformed code,  Up: Using ADiMat

4.6 Applying AD to an example
=============================

The above steps are applied to an example code now, the lighthouse
example which is introduced in the book by A. Griwank: *note
References:: . The function describing the coordinate where the beam of
a lighthouse hits a quay wall is given by:

     function y=lighthouse(nu, gamma, omega, t)
     % The lighthouse example from the book:
     % A. Griewank "Evaluating Derivatives" SIAM 2000 p. 16
     y(1) = (nu * tan(omega*t))/(gamma-tan(omega*t));
     y(2) = gamma*(nu * tan(omega*t))/(gamma-tan(omega*t));

   You could create function input parameters and run the function as
follows:

     n = 10; % (m)
     g = 0.375* pi; % (bogenmass)
     o = 0.0001* pi; % (bogenmass)
     t = 2; % (s)
     y = lighthouse(n, g, o, t);

   Look into the book for a more detailed description.

* Menu:

* Example using the high-level interface::
* Example doing the differentiation steps manually::
* Example using the scalar mode manually::


File: adimat.info,  Node: Example using the high-level interface,  Next: Example doing the differentiation steps manually,  Up: Applying AD to an example

4.6.1 Example using the high-level interface
--------------------------------------------

Using the high level interface you can obtain the derivative in forward
mode by entering one of the following two commands:

     [J y] = admDiffFor(@lighthouse, 1, n, g, o, t);
     [J y] = admDiffVFor(@lighthouse, 1, n, g, o, t);

   In forward mode, four derivative directions are needed, as there are
a total of four components in the four scalar inputs. The resulting
Jacobian matrix in J will have two rows and four columns.

   If you want to specify a seed matrix, pass it as the second
argument. The seed matrix must have four columns. The same result as in
the previous example can be obtained by manually passing the
four-by-four identity matrix:

     S = eye(4);
     [J y] = admDiffFor(@lighthouse, S, n, g, o, t);

   If you wanted to compute just the first column of the Jacobian, you
could pass a single column vector of length four, setting its first
argument to one:

     S = [1 0 0 0]';
     [J y] = admDiffFor(@lighthouse, S, n, g, o, t);

   However, in this case it is better to specify that only the first
parameter shall be independent, as the derivative code will by shorter:

     opts = admOptions('i', 1);
     [J y] = admDiffFor(@lighthouse, 1, n, g, o, t, opts);

   In both cases the the resulting J will be a two-by-one matrix, the
first column of the full Jacobian.

   For the reverse mode using the high level interface, type in the
following command:

     [J y] = admDiffRev(@lighthouse, 1, n, g, o, t);

   In reverse mode, only two derivative directions are needed, as there
are a total of two components in the single output variable y. The
resulting Jacobian matrix in J will of course again have two rows and
four columns.

   Again, if you want to specify a seed matrix, pass it as the second
argument. The seed matrix must have two rows. The same result as in the
previous example can be obtained by manually passing the two-by-two
identity matrix:

     S = eye(2);
     [J y] = admDiffRev(@lighthouse, S, n, g, o, t);

   If you wanted to compute just the first row of the Jacobian, you
could pass a single row vector of length two, setting its first
argument to one:

     S = [1 0];
     [J y] = admDiffRev(@lighthouse, S, n, g, o, t);

   In this case you cannot use the dependent option to do this, as
there is only a single output parameter.


File: adimat.info,  Node: Example doing the differentiation steps manually,  Next: Example using the scalar mode manually,  Prev: Example using the high-level interface,  Up: Applying AD to an example

4.6.2 Example doing the differentiation steps manually
------------------------------------------------------

You can also compute the full Jacobian in forward mode by running the
following commands:

     admTransform(@lighthouse); % produces g_lighthouse.m
     adimat_derivclass('opt_derivclass'); % select runtime environment
     % adimat_derivclass('arrderivclass'); % possible alternative
     S = eye(4); % create seed matrix
     [g_n g_g g_o g_t] = createSeededGradientsFor(S, n, g, o, t); % create derivative inputs
     [g_y y] = g_lighthouse(g_n, n, g_g, g, g_o, o, g_t, t); % run the differentiated function
     J = admJacFor(g_y); % extract derivative values

   Using the alternative forward mode implementation, this becomes:

     admTransform(@lighthouse, admOptions('m', 'f')); % produces d_lighthouse.m
     adimat_derivclass('vector_directderivs'); % select runtime environment
     S = eye(4); % create seed matrix
     [d_n d_g d_o d_t] = createSeededGradientsFor(S, n, g, o, t); % create derivative inputs
     [d_y y] = d_lighthouse(d_n, n, d_g, g, d_o, o, d_t, t); % run the differentiated function
     J = admJacFor(d_y); % extract derivative values

   Using the reverse mode, you need to run the following commands:

     admTransform(@lighthouse, admOptions('m', 'r')); % produces a_lighthouse.m
     adimat_derivclass('opt_derivclass'); % select runtime environment
     % adimat_derivclass('arrderivclass'); % possible alternative
     y = lighthouse(n, g, o, t); % run the original function (get y)
     S = eye(2); % create seed matrix
     [a_y] = createSeededGradientsRev(S, y); % create derivative inputs (adjoint of y)
     [a_n a_g a_o a_t y] = a_lighthouse(n, g, o, t, a_y); % run the differentiated function
     J = admJacRev(a_n, a_g, a_o, a_t); % extract derivative values


File: adimat.info,  Node: Example using the scalar mode manually,  Prev: Example doing the differentiation steps manually,  Up: Applying AD to an example

4.6.3 Example using the scalar mode manually
--------------------------------------------

In this example we will do as many steps as possible without using the
runtime environment functions. You can compute a certain directional
derivative along the direction four-vector [a b c d] in scalar mode by
running the following commands:

     admTransform(@lighthouse); % produces g_lighthouse.m
     adimat_derivclass('scalar_directderivs'); % select runtime environment for scalar mode
     g_n = zeros(size(n)); % create zero derivative inputs
     g_g = zeros(size(g)); % create zero derivative inputs
     g_o = zeros(size(o)); % create zero derivative inputs
     g_t = zeros(size(t)); % create zero derivative inputs
     g_n(1) = a; % set derivative direction
     g_g(1) = b; % set derivative direction
     g_o(1) = c; % set derivative direction
     g_t(1) = d; % set derivative direction
     [g_y y] = g_lighthouse(g_n, n, g_g, g, g_o, o, g_t, t); % run the differentiated function
     dirder = g_y(:); % extract derivative values

   Using the reverse mode, you can compute a certain reverse
directional derivative along the direction two-vector [a b] in scalar
mode by running the following commands:

     admTransform(@lighthouse, admOptions('m', 'r')); % produces a_lighthouse.m
     adimat_derivclass('scalar_directderivs'); % select runtime environment for scalar mode
     y = lighthouse(n, g, o, t); % run the original function (get y)
     a_y = zeros(size(y)); % create zero derivative inputs
     a_y(1) = a; % set derivative direction
     a_y(2) = b; % set derivative direction
     [a_n a_g a_o a_t y] = a_lighthouse(n, g, o, t, a_y); % run the differentiated function
     rdirder = [a_n a_g a_o a_t]; % extract derivative values

   If you want to compute the gradient of a scalar function f(a, b, c),
simply stick a one into the adjoint input parameter:

     admTransform(@f, admOptions('m', 'r')); % produces a_f.m
     adimat_derivclass('scalar_directderivs'); % select runtime environment for scalar mode
     [a_a a_b a_c z] = a_f(a, b, c, 1); % run the differentiated function
     gradient = [a_a(:).' a_b(:).' a_c(:).'];                        % concatenate derivative values into gradient


File: adimat.info,  Node: Using sparsity exploitation or compression,  Prev: Applying AD to an example,  Up: Using ADiMat

4.7 Using sparsity exploitation or compression
==============================================

In first order FM or RM, when the Jacobian matrix is sparse, i.e. it
has a lot of zero entries, then the number of directional derivatives
can often be reduced drastically, which is called `sparsity
exploitation'. This works by `compressing' the Jacobian matrix. The
non-zero pattern of the Jacobian is required for this.

   You can obtain the non-zero pattern P approximately by first
computing the full Jacobian J and then setting `P = J ~= 0'. This is
however slightly dangerous, because some entries in J may just happen
to be zero at the particular point where you evaluated the derivative,
but be non-zero at other points. You can be conservative in choosing P,
i.e. mark more components of J as non-zeros than necessary.

* Menu:

* Using sparsity exploitation with the high-level interface::
* Using sparsity exploitation manually::


File: adimat.info,  Node: Using sparsity exploitation with the high-level interface,  Next: Using sparsity exploitation manually,  Up: Using sparsity exploitation or compression

4.7.1 Using sparsity exploitation with the high-level interface
---------------------------------------------------------------

Using the high-level interface, you have to set the option field
JPattern to the non-zero pattern P. Then you can run admDiffFor as
usual, compression will automatically be used:

     opts.JPattern = P;
     admDiffFor(@f, 1, arg1, ..., argN, opts)

   The same thing works with admDiffVFor, admDiffRev, admDiffFD, and
admDiffComplex. The example above will implicitly use the conforming
identity matrix as the seed matrix, and compress it using the `coloring
heuristic' function cpr. This results in a compressed seed matrix,
which often has significantly fewer columns (fewer rows in RM), thereby
allowing for a faster derivative computation. The results is a
`compressed Jacobian', which will automatically be unpacked to a sparse
matrix, identical to J.

   The coloring heuristic cpr comes with ADiMat and implements the
Curtis-Powell-Reed heuristic. You can replace this heuristic by another
function with a similar signature.

   You can also use sparsity exploitation on top of a custom seed
matrix S. Also, you can set a custom coloring function using the option
`coloringFunction'.

     opts.JPattern = P;
     opts.coloringFunction = 'mycoloring';
     admDiffFor(@f, S, arg1, ..., argN, opts)


File: adimat.info,  Node: Using sparsity exploitation manually,  Prev: Using sparsity exploitation with the high-level interface,  Up: Using sparsity exploitation or compression

4.7.2 Using sparsity exploitation manually
------------------------------------------

If you compute derivatives repeatedly, but with a fixed non-zero
pattern, you may want to factor out the coloring and seed matrix
compression process from the differentiation process. This can be done
as follows:

     [cS coloring] = admColorSeed(P, opts); % construct compressed seed matrix
     cJac = admDiffFor(@f, cS, arg1, ..., argN, opts); % run differentiation
     Jac = admUncompressJac(cJac, P, coloring); % unpack the compressed Jacobian

   The same thing works with admDiffVFor, admDiffFD, and
admDiffComplex. You can also *note Running the transformed code::
manually. You could even use an entirely different FM AD tool to
compute the compressed Jacobian in the second line instead of ADiMat.

   In RM, however, you have to do something slightly different. Instead
of compressing columns of the seed matrix, you have to compress rows.
This can be achieved as follows, using transpose operations:

     [cS coloring] = admColorSeed(P.');                       % construct compressed seed matrix
     cJac = admDiffRev(@f, cS.', arg1, ..., argN, opts);      % run differentiation
     Jac = admUncompressJac(cJac.', P.', coloring); % unpack the compressed Jacobian
     Jac = Jac.';                                             % final transpose


File: adimat.info,  Node: Tips and recipes,  Next: The runtime environment,  Prev: Using ADiMat,  Up: Top

5 Tips and recipes
******************

We think ADiMat is now fairly complete, apart from the fact that there
are still many builtins which it cannot differentiate. However, our
feeling is that a large number of codes can be differentiated straight
away. In other cases, with some slight modifications you may be able to
differentiate your code succesfully. This section lists some common
practices.

* Menu:

* Differentiating functions with char int struct or cell parameters::
* Differentiating code in multiple directories::


File: adimat.info,  Node: Differentiating functions with char int struct or cell parameters,  Next: Differentiating code in multiple directories,  Up: Tips and recipes

5.1 Differentiating functions with char int struct or cell parameters
=====================================================================

ADiMat assumes that the input and output parameters of the function to
differentiate are double arrays. When an in- or output parameter is of
type char, int, struct of cell instead, some special measures must be
taken. In the following discussion we treat input parameters, output
parameters can be handled analogously.  We have to distinguish two
possible cases:

  1. The parameter is not an independent variable: if you specify this,
     there should be no problem

  2. The parameter is an independent variable: you should use a wrapper
     function

   In the first case, the parameter contains values, possibly strings,
maybe also numbers, that parametrize the function. Simply tell ADiMat
not to differentiate w.r.t. that parameter, using the independents
option field. For example, consider the following function: function z
= f(a, options)    if strcmp(options.mode, 'fast')       z = fast(a);
else       z = slow(a);    end    if options.postprocess == 1       z =
post(z);    end This function should be differentiated passing
admOptions('i', 1) as the options structure. This excludes the second
parameter from differentiation, and then there should be no problems.
In the second case, the parameter contains some float values which are
independent variables of your function. In this case you can write a
wrapper function, which receives the independent values of interest in
dedicated floating point parameters. The wrapper then places these
values in the correct location in the struct or cell array and calls
the original function. Consider the following example function:
function z = f(variables, options)    if strcmp(options.mode, 'fast')
   z = fast(variables.x * 2, variables.y * 0.5);    else       z =
slow(variables.x * 2, variables.y * 0.5);    end    if
options.postprocess == 1       z = post(z);    end The wrapper you
could use might look like this: function z = fwrap(x, y, variables,
options)    variables.x = x;    variables.y = y;    z = f(variables,
options); Instead of the original f, you should now differentiate
fwrap, passing admOptions('i', 1:2) as the options structure.


File: adimat.info,  Node: Differentiating code in multiple directories,  Prev: Differentiating functions with char int struct or cell parameters,  Up: Tips and recipes

5.2 Differentiating code in multiple directories
================================================

The high-level interfaces of ADiMat, e.g. admDiffFor, will find the
source code of the top-level function by using the *which* command, even
if it resides in a different directory. It will also automatically
append and appropriate -I switch to the command line, such that further
functions in that directory are also found. However, if some of the
functions called from the code reside in a second directory, you have
to add the corresponding -I switch manually.  Example: say your code is
in two directories, src/a and src/b. Function f is in src/a and calls
g, which is in src/b. For running your code you would use:
addpath('src/a'); addpath('src/b'); z = f(a, b); As stated, f.m can be
found automatically by admDiffFor via the which command. However,
ADiMat will then complain that g is an undefined identifier. You must
pass admOptions('f', ' -I src/b ') as the options structure to the AD
based differentiation routines admDiffFor et. al. Note this is not
necessary with admDiffFD or admDiffComplex.


File: adimat.info,  Node: The runtime environment,  Next: The derivative classes,  Prev: Tips and recipes,  Up: Top

6 The runtime environment
*************************

ADiMat comes with a set of Matlab functions. The most prominent are the
derivative classes, which are described in a seperate *note The
derivative classes:: . In this chapter we describe the funtions that
make up the ADiMat runtime environment.

* Menu:

* High level user interface functions::
* Interfaces to program transformation::
* Derivative and adjoint classes::
* Stacks for the reverse mode::
* Other helper functions for the reverse mode::


File: adimat.info,  Node: High level user interface functions,  Next: Interfaces to program transformation,  Up: The runtime environment

6.1 High level user interface functions
=======================================

`admDiffFor(function, S, arg1, ..., argN, opts?)'
     Differentiate function and compute the product J*S of the Jacobian
     matrix J at arguments arg1, ..., argN and the seed matrix S. The
     computational expense depends linearly on the number of columns of
     S.

     *admDiffFor(@f, 1, a, b, c)* returns the full Jacobian matrix of
     function f evaluated at the point (a, b, c). The value of 1 for the
     seed matrix is a shortcut for the identity matrix of the right
     size.

     *admDiffFor(@f, [1 1 1]', 1, 2, 3)* returns the column sum of the
     Jacobian matrix of function f evaluated at the point (1, 2, 3).

`admDiffVFor(function, S, arg1, ..., argN, opts?)'
     Differentiate function and compute the product J*S of the Jacobian
     matrix J at arguments arg1, ..., argN and the seed matrix S. The
     computational expense depends linearly on the number of columns of
     S.

     This function uses the new forward mode. Otherwise the
     functionality is the same.

`admDiffRev(function, S, arg1, ..., argN, opts?)'
     Differentiate function in reverse mode and compute the product S*J
     of the the seed matrix S and the Jacobian matrix J at arguments
     arg1, ..., argN. The computational expense depends linearly on the
     number of rows of S.

     *admDiffRev(@f, 1, a, b, c)* returns the full Jacobian matrix of
     function f evaluated at the point (a, b, c).

     *admDiffRev(@f, [1 1 1], 1, 2, 3)* returns the row sum of the
     Jacobian matrix of function f evaluated at the point (1, 2, 3).

`admTaylorFor(function, S, arg1, ..., argN, opts?)'
     Compute unvariate Taylor coefficients of function, with OO, see
     *note Higher order derivatives::

`admTaylorFor(function, S, arg1, ..., argN, opts?)'
     Compute unvariate Taylor coefficients of function, with FM ST, see
     *note Higher order derivatives::

`admTaylorRev(function, S, arg1, ..., argN, opts?)'
     Compute derivatives of unvariate Taylor coefficients of function,
     with OO applyied to function differentiated in RM, see *note
     Higher order derivatives::

`admHessian(function, S, arg1, ..., argN, opts?)'
     Compute Hessian of function, using one of several strategies, by
     default FM-over-RM, see *note Higher order derivatives::

`admHessFor(function, S, arg1, ..., argN, opts?)'

`admHessVFor(function, S, arg1, ..., argN, opts?)'

`admHessFD(function, S, arg1, ..., argN, opts?)'
     Compute Hessian of function via second order Taylor coefficients,
     see *note Higher order derivatives::

`admDiffFor2(function, S, arg1, ..., argN, opts?)'
     Compute Hessian of function via second order FM differntiation, see
     *note Higher order derivatives::

`admDiffComplex(function, S, arg1, ..., argN, opts?)'
     Differentiate function and compute the product J*S of the Jacobian
     matrix J at arguments arg1, ..., argN and the seed matrix S. The
     computational expense depends linearly on the number of columns of
     S.

     This function does not do AD at all, but instead uses the so called
     complex variable method to compute derivatives. The results are
     equally precise as those returned by AD, provided the function is
     real and analytic. This functions is provided in order to
     facilitate the comparison of AD results with the complex variable
     method.

`admDiffFD(function, S, arg1, ..., argN, opts?)'
     Differentiate function and compute the product J*S of the Jacobian
     matrix J at arguments arg1, ..., argN and the seed matrix S. The
     computational expense depends linearly on the number of columns of
     S.

     This function does not do AD at all, but instead uses divided
     differences to evaluate the derivatives approximately. The results
     are at best precise up to half of the machines matissa bits, but
     may be less precise when the differential step h is chosen
     carefully. Use the field ddStep of the admOptions structure to
     adjust the differential step h to your function. This functions is
     provided in order to facilitate the comparison of AD results with
     divided differences.

`admOptions(name1?, value1?, name2?, value2?, ...)'
     Create a structure that can be used to pass options to the admDiff*
     functions, as the last parameter, called `opts' in the synopses
     above.



File: adimat.info,  Node: Interfaces to program transformation,  Next: Derivative and adjoint classes,  Prev: High level user interface functions,  Up: The runtime environment

6.2 Interfaces to program transformation
========================================

`admTransform(function, indeps, deps, flags)'
     Matlab frontend to *adimat-client* (or *admproc*, if present).
     Differentiate `function' using ADiMat. `function' can be a
     filename as a string or a function handle. `flags' is a string
     which is added to the adimat-client command line. `indeps' and
     `deps' are strings containing comma separated lists of variable
     names or positions.

     admTransform works by executing the program adimat-client (or
     admproc, if present) via the *system* builtin. The builtin *which*
     is used to determine the path and filename of function.
     admTransform will automatically add the options -I and -p with
     appropriate arguments to the command line, for adding the
     directory where the file resides to ADiMat's search path and
     writing the transformed file to the same directory. You can
     override the output directory by providing your own -p option in
     flags.

     *admTransform(function)* transforms the function in classic
     forward mode.

     *admTransform(function, indeps, deps, '-F')* is the same.

     *admTransform(function, indeps, deps, '-f')* transforms the
     function in the new forward mode.

     *admTransform(function, indeps, deps, '-r')* transforms the
     function in the reverse mode.

     *admTransform(function, [1], [2], mode)* transforms the function
     such that you can compute the derivative of the second output
     parameter w.r.t. the first input parameter of the function.

`addiff(function, indeps, deps, flags)'
     Deprecated: This function is now just a synonym for admTransform.

`admrev(function, indeps, deps, flags)'
     Deprecated: This function is now just a synonym for admTransform.

`admClean(baseDir?, modes?)'
     This function deletes the ADiMat-generated functions from the
     directory `baseDir'. With the string `modes' you can choose which
     kind of files will be deleted. The letters in the string
     correspond to the mode flags (see *note Modes and Toolchains:: ),
     plus `'s" which refers to the status directory `.adimat'.

     Warning: Use this function with some care as the file name patterns
     might also match some other files!



File: adimat.info,  Node: Derivative and adjoint classes,  Next: Stacks for the reverse mode,  Prev: Interfaces to program transformation,  Up: The runtime environment

6.3 Derivative and adjoint classes
==================================

`adimat_derivclass(name)'
     Select the derivative class. This merely modifies the Matlab path.
     For a description of the classes, see *note The derivative
     classes:: . If `name' is the string 'list', a list of available
     classes is returned. With no argument, the name of the currently
     loaded derivative class is returned.

`adimat_adjoint(name)'
     Select the adjoint class. This merely modifies the Matlab path.
     There are two options, 'default' and 'scalar'. With no argument,
     the name of the currently loaded adjoint class is returned.

`g_zeros(sz)'
     This is the most prominent runtime function found in the
     transformed source code. Returns zero derivative objects of
     dimension sz of the derivative class that is currently in use (see
     adimat_derivclass). In the case of scalar_directderivs, simply
     returns zeros(sz), i.e. a plain double array.

`a_zeros(v)'
     When adimat_adjoint is 'default', then returns g_zeros(size(v)) to
     return zero adjoint objects. When adimat_adjoint is 'scalar' then
     returns zeros(size(v)), i.e. a plain double array.



File: adimat.info,  Node: Stacks for the reverse mode,  Next: Other helper functions for the reverse mode,  Prev: Derivative and adjoint classes,  Up: The runtime environment

6.4 Stacks for the reverse mode
===============================

`adimat_stack(name)'
     Select the stack implementation. All stacks are implemented via a
     single function adimat_store. This function switches the path to
     various directories where different implementations of adimat_store
     reside. If `name' is the string 'list', a list of available
     impletations is returned. With no argument, the name of the
     currently loaded stack is returned.

`adimat_push(v1, ...)'
     Push values on the stack. Takes any number of arguments.

`[v1, ...] = adimat_pop'
     Pop values from the stack. Returns any number of output arguments,
     but one value is always removed from the stack.

`adimat_store(mode, v?)'
     Operate on the stack according to mode:

        *  0 pop one value and return it

        *  1 push value v on the stack, return number of values on stack

        *  2 clear the stack, delete all values, return number of
          values on stack

        *  3 return size of stack

        *  4 return size of stack in bytes. Not always available, maybe
          0.

        *  5 flush the stack to disk. Not always available.

        *  6 return size of stack stored on disk, in bytes. Not always
          available, maybe 0.

`adimat_clear_stack'
     Calls adimat_store(2)

`adimat_stack_size'
     Calls adimat_store(3)

`adimat_stack_byte_size'
     Calls adimat_store(4)

`adimat_stack_file_size'
     Calls adimat_store(6)

`adimat_flush_stack'
     Calls adimat_store(5)

`adimat_stack_buffer_size(bytes)'
     Sets the size of the buffer used by the stack implementations with
     "buffered" in their name.

`adimat_aio_init(numThreads, numBuffers)'
     Sets the parameters of the aio.h API calling aio_init. Used by the
     stack implementations with "abuffered" in their name.

`adimat_stack_prefetch(n)'
     Sets a parameter of the aio.h based streambuffer, controlling how
     many preceding blocks of memory to prefetch when a read is being
     waited for.



File: adimat.info,  Node: Other helper functions for the reverse mode,  Prev: Stacks for the reverse mode,  Up: The runtime environment

6.5 Other helper functions for the reverse mode
===============================================

`adimat_adjred'

`adimat_multl, adimat_multm and adimat_multlr'


File: adimat.info,  Node: The derivative classes,  Next: Usage,  Prev: The runtime environment,  Up: Top

7 The derivative classes
************************

A number of derivative classes are available. At least one is needed to
support computation with derivative objects. The term 'derivative
class' is merely an abbreviation for the underlying functionality.  The
derivative class has to be seen as a package containing support
functions, constructor functions, interface wrappers and the Matlab
class needed to run a differentiated program.

   Which derivative class to use depends in part on how you produce the
differentiated code, and whether you want to run the differentiated
function in scalar or vector mode.

   Scalar mode means that only a single derivative direction is
computed. This implies that any derivative variable g_x have the same
dimensions as the variable x it is associated to. This case is
preferable as it allows to use doubles as derivative objects. For
example, if you have a scalar function f, function z = f(a, b, c) then
you could run the reverse mode function a_f by simply sticking a
literal 1 into the adjoint input parameter: gradient = a_f(a, b, c, 1)

   Vector mode means that several different derivative inputs are
concatenated. As in Matlab the objects may already be vectors, matrices
or tensors, the derivative objects have one more dimension than the
corresponding program variables. We use derivative classes to hide the
additional dimension, and do the correct derivative computations in the
overloaded operators of that class. In a different approach,
implemented by the command admproc -f and the derivative class
vector_directderivs we call special runtime functions with the same end.

   In the following list you find the possible combinations of
derivative code and derivative classes.

   *  Scalar mode

        *  Code produced with admTransform, mode 'F': use
          scalar_directderivs

        *  Code produced with admTransform, mode 'r': use
          scalar_directderivs, and adimat_adjoint('default') or any
          derivative class and adimat_adjoint('scalar')

   *  Vector mode

        *  Code produced with admTransform, mode 'F': use
          arrderivclass, arrderivclassvxdd, opt_derivclass,
          opt_sp_derivclass or mat_derivclass

        *  Code produced with admTransform, mode 'f': use
          arrderivclass, arrderivclassvxdd, opt_derivclass,
          opt_sp_derivclass or mat_derivclass and
          adimat_adjoint('default')

        *  Code produced with admTransform, mode 'f': use
          vector_directderivs

   All installed derivative classes can be found in the directory
'${ADIMAT_HOME}/share/adimat/'. The desired class can be selected at
runtime using the function *note Derivative and adjoint classes:: .
Using the driver functions like admDiffFor, etc., the option
derivClassName can be used to select the derivative class. The
following derivative classes are available:

`arrderivclass'
     In this class, derivatives are stored internally in one big tensor
     array of size [NDD size(x)], where x is the corresponding original
     variable. With this class many operations very fast, because no
     loop over NDD is required. On the other hand, some other
     operations like cat, horzcat, vertcat, mtimes and mldivide are
     slower than with opt_derivclass. Also, some indexing operations may
     not be correctly supported by this class. In this case, please
     provide us with an example so we can try to fix the issue. This
     derivative class is used by default.

`arrderivclassvxdd'
     Like arrderivclass, but the internal array has a different layout.
     It is always two-dimentional of size [prod(size(x)), NDD].

`foderivclass'
     This derivative class is written using the new classdef construct
     in Matlab, so it will not work in Octave 3.6. Otherwise it is
     identical to arrderivclass.

`opt_derivclass'
     Sometimes also called opt3_derivclass, because it is the third
     version of a derivative class based on cell arrays. This
     derivative class comprises the whole set of operators needed for
     computation with first order and second order derivative objects
     (gradients and Hessians). This derivative class is well maintained
     and mostly stable. It is suitable for programs whose derivatives
     are known to be full. That is, the derivative objects have less
     than 70% zeros. Allthough this class supports sparse derivative
     objects, the derivatives are not converted back to sparse data
     structures after operations, which return full matrices like
     mtimes.

`opt_sp_derivclass'
     Also called opt3_sp_derivclass. Functional identically with the
     opt3_derivclass. Features conversion of directional derivatives
     with more than 70% of zero entries to sparse data structures
     conserving memory and computational resources. Note, that it
     possibly is slower than the opt3_derivclass if many non-zero
     entries are present in the directional derivatives.

`mat_derivclass'
     Uses a matrix for storing directional derivatives instead of the
     cell array the opt3_derivclass uses. The complete set of operators
     is available but only for first order derivatives currently (i.e.
     no Hessian computations are possible). Because one level of
     indirection is missing (no access of a cell array), this class is
     faster. It is speed up further for certain operations because the
     operation is not applied every directional derivative
     successively, but to all directional derivatives at once.

`scalar_directderivs'
     This is not really a class but only a collection of runtime
     functions. Most importantly it has a version of *note Derivative
     and adjoint classes::  that returns native doubles. Note that it
     does not have the ls_* runtime functions, so adimat should be run
     with option *note adimat Options::  to produce the code.

`vector_directderivs'
     This is not really a class but only a collection of runtime
     functions. The derivative object of an m x n double object is an d
     x n x m double object. Currently this derivative class is only for
     use with code produced by admproc -f, and that code can only be
     run with this derivative class.


   The creation of derivative objects for all derivative class is done
using constructor functions. These functions create seedings that are
often used. See section *note Constructor functions `create---()'::
for more information on all available functions.

* Menu:

* Structure of derivative objects::
* Creation of derivative objects::
* Accessing the derivative data::
* Constructor functions `create---()'::
* The options system of the Matlab-class::
* Matlab's cellarrays/structures and the derivative class::


File: adimat.info,  Node: Structure of derivative objects,  Next: Creation of derivative objects,  Up: The derivative classes

7.1 Structure of derivative objects
===================================

A derivative object should be regarded as a container storing
directional derivatives. Derivative objects are associated to
Matlab-objects, but do not store references to them. The association is
by name only.

   A derivative object stores a number of objects in it that have the
same shape as the associated Matlab-object. E.g., the derivative object
`g_t' associated to a 3x3-matrix `t' stores a number of 3x3-matrices.
The number of 3x3-matrices stored in the derivative object is defined by
the number of directional derivatives of interest in the program.

   Derivate objects may be one- or two-dimensional. One-dimesional
derivative objects are called gradients or Jacobians depending on the
context, while two-dimensional derivative objects are called Hessians.

   The data within a derivative object is accessed using the standard
Matlab cellarray-assignment- and indexing-operators. This is
independent of the implemented storage model. The names of the actual
derivative class vary.  For example, the name of the Matlab-class of
the opt_derivclass is adderiv, the one of the opt_sp_derivclass is
adderivsp, and for the mat_derivclass madderiv. These names occur in
the list displayed by the Matlab's 'whos' command, if derivative
objects are present in the current workspace.  Conventionally only one
kind should occur. Intermixing them is not supported and may need
manual conversion if desired.


File: adimat.info,  Node: Creation of derivative objects,  Next: Accessing the derivative data,  Prev: Structure of derivative objects,  Up: The derivative classes

7.2 Creation of derivative objects
==================================

Derivative objects are created using constructor functions.
'`createZeroGradients()" is one of them. The function is able to
initialise several derivative objects at once and may be called several
times.  If calling the function several times, the number of
directional derivatives has to be the same in each call. Additional
constructor functions exist, see section *note Constructor functions
`create---()':: .

`Function:'
          [g_v1, g_v2, ..., g_vn]= createZeroGradients(ndd, v1, v2, ..., vn);

     or

          g_v1= createZeroGradients(ndd, v1);
          g_v2= createZeroGradients(ndd, v2);
                   ...
          g_vn= createZeroGradients(ndd, vn);

`Short description:'
     Initialise one or more derivative objects.

`Description:'
     This function initialises one or more derivative objects. The
     number of directional derivatives created per derivative object is
     denoted by the parameter '`ndd". If using the vectorised call --
     the upper one --, the order of the variables '`vi" and the order
     of the corresponding derivative objects have to be ensured by the
     user. There is no way to ensure this automaticaly or check for a
     proper order.  The derivative object '`g_vi" of the variable '`vi"
     stores '`ndd" many copies of the variable '`vi", but all entries
     are set to zero. That is why this function is named
     `create*Zero*Gradients()'.  All derivative objects have to store
     the same number of directional derivatives. It is therefore
     advised to use the vectorised function, which ensures that all
     derivative objects have the same number. It may be possible to
     change the number of directional derivatives in future version of
     ADiMat, but upto now this is not supported. Messing around with
     the number of directional derivatives during one run of the
     differentiated program is done on your own risk, so do not
     complain about wrong derivatives.

`Examples:'
     The lighthouse example, which uses scalars only:

          n= 10; % (m)
          g= 0.375* pi; % (bogenmass)
          o= 0.0001* pi; % (bogenmass)
          t= 2; % (s)
          [g_n, g_g, g_o, g_t]= createZeroGradients(4, n, g, o, t);

     The derivative objects are all initialised to zero now. The
     contents of '`g_n" is:

          >> g_n
          adderiv: number of directional derivatives: 4
               0
               0
               0
               0

     Each line containing a zero shows one directional derivative. The
     example above is to simple to see the effect, therefore a more
     complex one is introduced here. Suppose a row-vector '`v"
     containing five float numbers and a scalar '`s" are the
     independent variables a function is differentiated with respect
     to. The derivatives of interest are the first three entries of the
     vector and the scalar. That is, four directional derivatives are
     needed. The call to the constructor function is given by:

          >> v= [1, 2, 3, 4, 5]; % Same like 1:5
          >> s= 42;
          >> [g_v, g_s]= createZeroGradients(4, v, s);
          >> g_v
          adderiv: number of directional derivatives: 4
               0 0 0 0 0
               0 0 0 0 0
               0 0 0 0 0
               0 0 0 0 0
          >>

     The output of the gradient object of '`g_v" is shown above. There
     are four row-vectors each containig five zeros. This is because
     the original vector had 5 entries and four directional derivatives
     are of interest. The output of '`g_s" is identical to the output
     of '`g_n" shown in the lighthouse example above.



File: adimat.info,  Node: Accessing the derivative data,  Next: Constructor functions `create---()',  Prev: Creation of derivative objects,  Up: The derivative classes

7.3 Accessing the derivative data
=================================

Each derivative object is an object of a Matlab-class provided by
ADiMat.  The class overloads several operators and (re-)implements some
functions. The cellarray-indexing-operator '{n}' accesses single,
multiple or all directional derivatives in a derivative object.
Additionally a '`get()" method is implemented, which basically does the
same job. The advantage of the '`get()" method is, that access to all
directional derivatives is implement in a performant way. The drawback
of the `get()' method is, that it implements a restricted set of
adressing only.  I.e., it is not possible to select the third
derivative of an object and get ist (2,3)-th element in one statement.

`Operator:'
          g_v{n}= ...;

     or

          t= g_v{n};

`Short description:'
     Write or read data of the n-th directional derivative of an object
     '`g_v".

`Description:'
     The cellarray-indexing-operator either in assigning mode or in
     referencing mode is used to assign data to a directional
     derivative or to read it, respectively. If the expression
     '`g_v{n}" appears on the left-hand side of an assignment the
     cellarray-indexing-operator is in assigning mode. If the
     expression occurs on the left-hand side of an assignment or in no
     assignment at all, it is treated to be in referencing mode.
     Indexing operators may be concatenated. Suppose that v is a higher
     dimensional object, a vector for example. The expression
     'g_v{i}(j)' accesses the j-th element of the i-th directional
     derivative of the object '`g_v". This is possible in assigning and
     in referencing mode.

`Examples:'
     The cellarray-indexing-operators are often used to do the seeding
     or to look at one directional derivative. The example presented
     with the `createZeroGradient()'-function is repeated here to show
     one possible seeding to get the desired derivatives. Remember the
     derivatives of interest are the first three entries of the vector
     v and the scalar. Create the derivative objects first:

          >> v= [1, 2, 3, 4, 5]; % Same like 1:5
          >> s= 42;
          >> [g_v, g_s]= createZeroGradients(4, v, s);

     The seeding is done by inserting ones at the desired positions of
     the derivative objects:

          >> g_v{1}(1)= 1;
          >> g_v{2}(2)= 1;
          >> g_v{3}(3)= 1;
          >> g_s{4}= 1;
          >> g_v
          adderiv: number of directional derivatives: 4
               1 0 0 0 0
               0 1 0 0 0
               0 0 1 0 0
               0 0 0 0 0
          >>g_s
          adderiv: number of directional derivatives: 4
               0
               0
               0
               1
          >>

`Function:'
          r_i= get(g_v, index);

     or

          r_all= get(g_v, 'direct');

     or

          opt= get(g_v, optionname);

`Short description:'
     Get some or all directional derivatives of a derivative object or
     get values of options.

`Description:'
     The `get()'-method gets single directional derivatives, multiple
     directional derivative, or all. Depending on the underlying
     derivative class using get() to extract all directional derivatives
     from a derivative object is more efficient, because the
     get()-method does not truncate the directional derivatives when
     extracting them, but returns them in a matrix.

     To extract all directional derivatives from a derivative object
     `g_v' it is strongly advised to used `r_all= get(g_v, 'direct')'.
     This special form merely copies the internal matrix of the
     directional derivative to the result matrix `r_all'. The
     directional derivatives are concatenated horizontally, if the
     directional derivative is one dimensional, i.e. first order
     derivatives are computed, and are stored matrix-like if second
     order derivatives are computed.  At last, the `get()'-method
     enables reading of internally stored options. To find out which
     options are by the derivative class, look at the help text of get.
     Make sure that you preceed the name get by the correct name of the
     derivative class you are using. That is, if you use the
     `opt_derivclass' then the command `help adderiv/get' show the
     correct help text. More information on the options system is
     available in the section *note The options system of the
     Matlab-class:: .

`Examples:'
     The example settings used in the example of the operator '{n}' are
     reused here. I.e., the derivative objects `g_v' and `g_s' are
     assumed to exist.

          >> get(g_v, 1)
          ans =
              1 0 0 0 0
          >> get(g_v, 'direct')
          ans =
            Columns 1 through 13
               1 0 0 0 0 0 1 0 0 0 0 0 1
            Columns 14 through 20
               0 0 0 0 0 0 0
          >>

     These two examples show the fetch of the first directional
     derivative of `g_v' and the fetch of all directional derivatives
     of `g_v'.



File: adimat.info,  Node: Constructor functions `create---()',  Next: The options system of the Matlab-class,  Prev: Accessing the derivative data,  Up: The derivative classes

7.4 Constructor functions `create--()'
======================================

ADiMat provides some functions to easily create one or more derivative
objects. The -- so called -- constructor functions provided, create
derivative objects with all elements set to zero, the diagonal of all
ojects set to one, and to create the full Jacobian.

`Function:'
          [g_v1, g_v2, ..., g_vn]= createZeroGradients(ndd, v1, v2, ..., vn);

     or

          g_v1= createZeroGradients(ndd, v1);
          g_v2= createZeroGradients(ndd, v2);
                   ...
          g_vn= createZeroGradients(ndd, vn);

`Short description:'
     Initialise one or more derivative objects with zeros.

`Description:'
     This function initialises one or more derivative objects. The
     number of directional derivatives created per derivative object is
     denoted by the parameter '`ndd". If using the vectorised call --
     the upper one -- , the order of the variables '`vi" and the order
     of the corresponding derivative objects have to be ensured by the
     user. There is no way to ensure this automaticaly or check for a
     proper order.  The derivative object '`g_vi" of the variable '`vi"
     stores '`ndd" many copies of the variable '`vi", but all entries
     are set to zero. That is why this function is named
     `create*Zero*Gradients()'.  All derivative objects have to store
     the same number of directional derivatives. It is therefore
     advised to use the vectorised function, which ensures that all
     derivative objects have the same number. It may be possible to
     change the number of directional derivatives in future version of
     ADiMat, but upto now this is not supported. Messing around with
     the number of directional derivatives during one run of the
     differentiated program is done on your own risk, so do not
     complain about wrong derivatives.

`Examples:'
     Have a look at *note Creation of derivative objects:: .

`Function:'
          [g_v1, g_v2, ..., g_vn]= createFullGradients(v1, v2, ..., vn);

`Short description:'
     Create full Jacobian for all `vi'.

`Description:'
     Creates derivative objects for all `vi'. The number of the
     directional derivatives stored in each `g_vi' is computed from the
     sum of the product of the sizes of all `vi'. Or to spell it in
     speudo-Matlab: `ndd=sum(prod(size(vi))) for i=1:n'. The seeding is
     done in a way, that the derivatives are computed with respect to
     each input element.  The function is restricted to arrays for
     inputs. I.e. structures and cellarrays are rejected. This function
     can be called once, only, in a program, or after reseting the
     ADoptions (see *note The options system of the Matlab-class:: ).

`Example:'
          >> t=magic(3);
          >> g_t=createFullGradients(t)
          adderiv: number of directional derivatives: 9
               1 0 0
               0 0 0
               0 0 0
               0 1 0
               0 0 0
               0 0 0
               0 0 1
               0 0 0
               0 0 0
            ... and so on ...
               0 0 0
               0 0 0
               0 1 0
               0 0 0
               0 0 0
               0 0 1

`Function:'
          [g_v1, g_v2, ..., g_vn]= createEyeGradients(v1, v2, ..., vn);

`Short description:'
     Create derivatives with the diagonal elements set to one.

`Description:'
     Creates derivative objects for all `vi'. The number of the
     directional derivatives is the sum of minimum of the size of each
     `vi', or in pseudo-Matlab: `ndd=sum(min(size(vi))) forall i=1:n'.
     This function can be applied only once per program, or after
     reseting the ADoptions (see *note The options system of the
     Matlab-class:: ).

`Example:'
          >> t=magic(3);
          >> g_t=createEyeGradients(t)
          adderiv: number of directional derivatives: 3
               1 0 0
               0 0 0
               0 0 0
               0 0 0
               0 1 0
               0 0 0
               0 0 0
               0 0 0
               0 0 1

`Function:'
          [h_v1, h_v2, ..., h_vn]= createHessians([ndd ndd], v1, v2, ..., vn);

     or

          [h_v1, h_v2, ..., h_vn]= createHessians([], v1, v2, ..., vn);

`Short description:'
     Create Hessians.

`Description:'
     Creates twodimensional derivative objects for all `vi'.  The
     number of the directional derivatives stored in each `h_vi' is
     specified by `[ndd ndd]' or if the empty matrix is supplied, taken
     from the options-system (see *note The options system of the
     Matlab-class:: ). The dimension of a `h_vi' object is ensured to
     be twodimensional.  Each `h_vi' contains sparse objects with all
     elements set to zero.

`Example:'
          >> t=magic(3);
          >> h_t= createHessians([], t)
          adderiv: total number of directional derivatives: 3x3
            (1,:)
            (2,:)
            (3,:)

`Functions:'
          g_v= g_zeros(size(v));
          h_v= h_zeros(size(v));

`Short description:'
     Create one/two-dimensional derivative objects with all elements
     set to zero.

`Description:'
     These functions create zero-objects. They are for internal use
     only. Essentially, the functions give the derivative of a Matlab's
     `zeros(),ones(),eye()...' matrix constructor functions. The call
     of these functions is insert everywhere, where no derivative can
     be computed. This may be the cause for the assignment of constant
     arrays (scalars).  Direct use of these functions is not recommend,
     because the functions may change without further notice!



File: adimat.info,  Node: The options system of the Matlab-class,  Next: Matlab's cellarrays/structures and the derivative class,  Prev: Constructor functions `create---()',  Up: The derivative classes

7.5 The options system of the Matlab-class
==========================================

The options system implements a way to store global information needed
by the process of computing derivatives invisibly. Traditionally, a
flag used by Matlab's toolboxes is stored in the global workspace,
where it is lost after a call to `clear all'. The information
maintained by the options systems survives, because it is stored as
persistent data in a private member-function. (If you did not
understand the previous sentence, do not bother anymore, because it was
developer language).

`Function:'
          val=get(g_t,optionname);

`Short description:'
     Get the option's value specified by `optionname' or clear all
     options setable.

`Description:'
     Get the value of a specific option. Use `help adderiv/get' to get
     a list of available options. The object supplied as `g_t' needs to
     be an object of the current derivative class. A multi purpose
     object is available by supplying `g_dummy' for `g_t'. Some options
     are local to an object, some are global. Wether an option is local
     or global is documented in the help-text of the get()-method.  A
     special option is specified by '`ClearAll". This option resets all
     options setable in the options system to their default values.
     This option may be needed, if a program needs to manipulate the
     number of directional derivatives that are stored in a derivative
     object by default. Allthough computations on derivative objects
     storing distinct numbers of directional derivatives is not
     supported by the derivative class, in some cases the number of
     directional derivatives needs to be reset, if for instance another
     program is to be executed, featuring a distinct number of
     directional derivatives.

`Example:'
          >> get(g_dummy, 'NumberOfDirectionalDerivatives');
          ans =
                3
          >> ver=get(g_dummy, 'Version')
          ver =
              0.5000
          >>

`Function:'
          set(...)

`Short description:'
     Exists, but hands off. Internal use, only.

`Description:'
     This function is intentionally undocumented. It is for internal
     use only. Messing around with it, will cause unexpected behaviour.



File: adimat.info,  Node: Matlab's cellarrays/structures and the derivative class,  Prev: The options system of the Matlab-class,  Up: The derivative classes

7.6 Matlab's cellarrays/structures and the derivative class
===========================================================

Matlab implements two datatypes that get special treatment by the
derivative class. The first one is the cellarray-datatype and the
second one are structures.

`cellarray: { }'
     Cellarrays are able to store objects of different types. I.e. a
     cellarray may store a string, an array, and a scalar. The
     cellarray is organized like a standard Matlab-array. I.e., it is
     indexable.  In conjunction with the derivative class: A cellarray
     can never be stored within a derivative object. But the derivative
     object can be stored within the cellarray. In fact, there is no
     need to modify codes containing cellarray when they are to be
     differentiated by ADiMat. The source transformation component
     ensures correct treatment of the cellarray and the derivative
     objects. The only issue to take off is the access of the data. The
     derivative object is in the cellarray. I.e. the first index
     accesses the derivative object, the second the data within it.
     Example: Let `a', `b', and `c' be active variables and
     `ca={a,b,c}' be the cellarray combining them to a vector. The
     derivative expression for `ca' is: `g_ca={g_a,g_b,g_c}'.  The
     expression `g_ca{1}' will access the first derivative object in
     the vector, namely `g_a'. To access the second component of first
     item of `g_ca' the expression: `g_ca{1}{2}' has to be used.

`structures: struct.field'
     Structures enable the storage of distinct data in a hierarchical
     way. The source transformation component of ADiMat ensures that
     the base object, i.e.  the variable storing the structure, is a
     derivative object. This has to be taken into account when creating
     structures that are active and the derivative has to created. At
     first a dummy-derivative objects has to be created and then the
     fields have to be inserted. In this way a structure is stored
     within a derivative object. The other way around, a derivative
     object is not to be stored within a structure. This is enforced,
     because during the activity analysis of the source transformation
     component the variable containing the structure is taken into
     account only and the fields are of no interest.  Derivative
     objects of structures may created using the constructor functions
     *note Constructor functions `create---()'::  for gradients and
     Jacobians and *note Constructor functions `create---()'::  for
     Hessians.  The constructor functions `createFullGradients()' and
     `createEyeGradients()' can not be used to create a derivative
     object for a structure.  Example: `str.field1=[1, 2]
     str.field2=42;' create a simple structure. The constructor
     function `g_str= createZeroGradients(3, str);' creates a suitable
     derivative object:

          >> g_str
          adderiv: number of directional derivatives: 3
              field1: [0 0]
              field2: 0
              field1: [0 0]
              field2: 0
              field1: [0 0]
              field2: 0



File: adimat.info,  Node: Usage,  Next: Usage of classic adimat,  Prev: The derivative classes,  Up: Top

8 Usage
*******

* Menu:

* Modes and Toolchains::
* List of toolchains::
* Transformation parameters::
* List of transformation parameters::


File: adimat.info,  Node: Modes and Toolchains,  Next: List of toolchains,  Up: Usage

8.1 Modes and Toolchains
========================

ADiMat defines several different modes of source code transformation,
which are called `toolchains'. The most prominent are the
transformation in forward mode and reverse mode of AD, but there are
others. All can be accessed from within Matlab via the admTransform
command, from the command line via the adimat-client program or via the
web interface.

   When you use the admTransform function, give it the name of handle of
your function and an admOptions structure. In this structure the fields
`mode' and `toolchain' allow you to choose the toolchain. The four AD
transformations are set via the field mode:

   * `'F" - perform forward mode AD transformation by the classic ADiMat
     implementation. This is used by *admDiffFor*

   * `'f" - run the toolchain "forward-ad", which perform the new
     forward mode AD transformation, as it is used by *admDiffVFor*

   * `'r" - run the toolchain "reverse-ad", which performs the reverse
     mode AD transformation, as it is used by *admDiffRev* and
     *admTaylorRev*

   * `'t" - run the toolchain "taylor-ad", which performs the new
     forward mode AD transformation for progagating univariate Taylor
     coefficients, as it is used by *admTaylorFor*

   The other toolchains are selected by specifying their name, as it is
given in the list of toolchains (cf.  *note List of toolchains:: ). The
toolchain name also appears on the web interface.

   When using `adimat-client', the four AD modes are selected by the
short flags `-F', `-f', `-r', and `-t', with the same meaning as in the
list above. The classic forward mode AD is the default, so the `-F'
flag may be omitted. The other toolchains are selected by giving the
name to the option `-T'.

   On the web interface of the ADiMat transformation server you can
choose the toolchain in the select field which is in the fieldset
labeled "Select transformation". Selecting a toolchain will show a
corresponding fieldset. These will automatically show fields for the
mandatory parameters. Other parameters can be added by clicking on the
button "Add a parameter".


File: adimat.info,  Node: List of toolchains,  Next: Transformation parameters,  Prev: Modes and Toolchains,  Up: Usage

8.2 List of toolchains
======================

This is a provisional list of the most important parameters. For more
information please refer to the web server interface.

`classic-forward-ad'
     This is used by admDiffFor.
     *Parameters*:

    `gradprefix=<PREFIX>'
          This prefix is added to each active variable. The prefix is
          used for variables, when ADiMat is in forward mode
          (<PREFIX>='g_').

    `hessprefix=<PREFIX>'
          If the `2ndorderfwd' ( *note adimat Options:: ) flag is set,
          then this prefix is used to preceed the variables that store
          Hessians (<PREFIX>='h_').

    `funcprefix=<PREFIX>'
          The prefix is added to each function that is augemented in
          forward mode. There is no distinction between functions that
          are differentiated once or twice. (<PREFIX>='g_')


`forward-ad'
     This is used by admDiffVFor.

     *Parameters*:

    `prefix=<d_>'
          This prefix is added to each active variable. The prefix is
          used for variables and functions.

    `forward-deriv-variable-prefix=<PREFIX>'
          The prefix used for differentiated variables (default:
          <PREFIX>=$prefix).

    `forward-deriv-function-prefix=<PREFIX>'
          The prefix used for differentiated functions (default:
          <PREFIX>=$prefix).

    `use-subsref=<1|0>'
          Set to one to wrap index operations on RHS in calls to the
          adimat_opdiff_subsref function (default: 0). Try to turn this
          on when you get errors regarding dimension mismatches or
          wrong sizes. However, code with this option on is much
          slower. See *note Questions and Answers:: .

    `use-subsasgn=<1|0>'
          Set to one to wrap assignments to index expressions in calls
          to the adimat_opdiff_subsasgn function (default: 0). Try to
          turn this on when you get errors regarding dimension
          mismatches or wrong sizes. However, code with this option on
          is much slower. See *note Questions and Answers:: .


`reverse-ad'
     This is used by admDiffRev.

     *Parameters*:

    `rev-prefix=<a_>'
          Set the prefix of adjoint functions and variables.

    `rec-prefix=<a_>'
          Set the prefix of recording (forward sweep) functions.

    `ret-prefix=<a_>'
          Set the prefix of returning (reverse sweep) functions.


`brackets-to-hvcat'
     Translate instances of brackets [] to calls to the functions
     horzcat and/or vertcat.

`hvcat-to-brackets'
     Rewrite calls of functions horzcat and/or vertcat with brackets [].

`ind-to-subs'
     Translate index expression to calls to the functions subsref
     (RHSs) and subsasgn (LHSs).

`for-insert-iteration-indexvar'
     Canonicalize for loops to the `for i=first:last'.

`null'
     Returns the raw XML output of the parser in the adimat tool.
     Results of activity analysis for each variable and the function
     call graph are also included.

`rename'
     Renames all identifiers given by parameter "from" to the value of
     parameter "to".



File: adimat.info,  Node: Transformation parameters,  Next: List of transformation parameters,  Prev: List of toolchains,  Up: Usage

8.3 Transformation parameters
=============================

ADiMat uses several parameters that can be used to influence certain
aspects of the source code transformation. The set of parameters is
different for each transformation toolchain.

   Transformation parameters are set using the sub struct parameters of
the admOptions structure. This is contructed by the function
admTransformParameters.  adopts = admOptions;
adopts.parameters.useSubsref = '1';

   Transformation parameters can also be set on the command line of
adimat-client, by passing a string of the form name=value to the option
-s, for example like this: adimat-client -f -o- -s use-subsref=1 f.m

   Parameter names are slightly different in both cases: in the
structure, the parameter names are written in camel-style, e.g.
"useSubsref", or "printStackInfo". On the command-line, dashed style is
used instead: "use-subsref", or "print-stack-info".  Parameter name are
translated automatically when the command-line is constructed by
admTransform.


File: adimat.info,  Node: List of transformation parameters,  Prev: Transformation parameters,  Up: Usage

8.4 List of transformation parameters
=====================================

This is a provisional list of the most important parameters. For more
information please refer to the web server interface. For any toolchain
you select on the web server, when you hit the button "Add a
parameter", you will see only the parameters that are relevant for that
toolchain.


File: adimat.info,  Node: Usage of classic adimat,  Next: Directives,  Prev: Usage,  Up: Top

9 Usage of classic adimat
*************************

The classic adimat binary is still in use on the transformation server
and will be available to those users who obtain a standalone version.

   The options and flags of the adimat program as they are listed below
can be specified using transformation parameters. Basically, fields of
the corresponding names can be found in the `parameters' sub struct of
*admOptions* (see *note Transformation parameters:: ). Parameters can
also be given in the command line to *adimat-client*. In general, an
option like `-gradprefix=GG_' should be specified to adimat-client as
`-s gradprefix=GG_'. A flag `-tmpclear' should be specified to
adimat-client as `-s tmpclear=1' or just `-s tmpclear'. You can also
use a prefix `no' to negate a flag. That is, `-s tmpclear=0', `-s
notmpclear', `-s notmpclear=1', and even `-s nonotmpclear=0', are all
equivalent.

   The usage of adimat is:

     adimat [options] inputfilename

   adimat produces code differentiated in forward mode AD. If no output
filename is given, then the differentiated files will be prefixed with
g_.  Most options have a short and a long name. This way '-h' and
'-help' have the same meaning. (Long options are prefixed by two '-',
only in case it is undistinguishable in the printout.)  The available
options with the same meaning are enumerate here. A commata ',' is used
to seperate options having the same meaning. Directives are named with
their corresponding option embracing them in square brackets [...].
Default values are written in braces (<N>=6).

* Menu:

* adimat Options::


File: adimat.info,  Node: adimat Options,  Up: Usage of classic adimat

9.1 adimat Options
==================



    `-h, -?, --help'
          Prints a help message and exits the application.

    `--version'
          Print the current version of ADiMat.

    `-v<N>, --verblevel=<N>'
          Set the verbositylevel to <N>(<N>=6) ADiMat's output routines
          know a level based system for issuing messages depending on
          how much the user wants to know. The availabe level range
          from 0 -> no output at all upto 12 -> output every single
          step. The table below gives a summary of the available
          outputlevels:


              `0'
                    no output at all

              `1'
                    output only critical errors (like no memory,...)

              `2'
                    errors which can be recovered by stepping over the
                    erroneous instruction, but this may cause more
                    errors.

              `4'
                    write warnings for everything that is not clear to
                    ADiMat or may result in unexpected programbehaviour

              `6'
                    [the default] print a small version information at
                    programstart and some statistics at
                    programmtermination

              `8'
                    print every major program step

              `10'
                    print every minor program step

              `12'
                    prints most information. This will print a real lot
                    of lines.  Do not blame me, if it clutters your
                    screen.


          The higher numbers include the lesser ones. All output is
          written to standard error.

    `-p<OUTPUTDIR>, --outputpath=<OUTPUTDIR>,'
          [AD_OUTPUT_DIR="<OUTPUTDIR>"] The path where ADiMat puts the
          augmented files. (<OUTPUTDIR>='.')  ADiMats default is the
          local directory, but it is not really clever to use it,
          because ADiMat will produce a huge number of files. It may
          overwrite some existing files, too. Using a subdir is highly
          encouraged. The subdir has to exist, it will not be created.

    `-d<DVARLIST>, [AD_DVARS=<DVARLIST>]'
          Specify the list of dependent variables. This flag is only
          read if the `inputfilename' specifies a file containing a
          function. The flag can be used to select some or all of the
          variables of the result list of the function to be dependent
          variables. If the flag is not specified and the inputfilename
          specifies a function, then all results are treated to be
          dependent variables.  *The command line switch overrides the
          directive AD_DVARS!!!*

    `-i<IVARLIST>, [AD_IVARS=<IVARLIST>]'
          Like -d but for the independent variables. All parameters are
          selected to be independent, if the inputfilename is a
          function and the flag is not given.  *The command line switch
          overrides the directive AD_IVARS!!!*

    `-I<PATH>'
          Add the directories given in <PATH>to the path, where ADiMat
          looks for matlab m-files. The option -I<PATH> may occur more
          than once. The valid paths of each occurrence of -I<PATH>
          will be appended to the list of directories scanned for
          missing functions.  Invalid paths will be discarded.
          Environments variables are valid and will be expanded. The
          special tag '@BUILTINS' may be inserted anywhere, but it is
          not allowed to habe more than one occurrence of it in the
          list. <PATH> may contain multiple directories seperated by
          colons ':'.  (<PATH>='.:@BUILTINS')

    `--noforwardvarresolv, [GLOBAL NOFORWARDVARRESOLVE]'
          This options prevents that ADiMat looks forward in the code
          to learn, if an unknown identifier is a variable. For example
          in: 'b=a; a=5;' the 'a' in 'b=a' is bound to the same 'a' as
          in 'a=5'. This, of course, is an error in this case, because
          'a' was not defined before use. The flag exists for the rare
          situations, where a function has the same name, like a
          variable and the variable is defined later on. Consider this
          bad programming style.

    `--writescopefile, [GLOBAL WRITESCOPEFILE]>'
          Write the file '<OUTPUTDIR>/adimat.dbg' which contains
          information about the scopes and the identifiers bound in
          them. The file contains a history of the identifier
          resolution process. This file is for debugging purpose only,
          it is not needed by the differentiated program. If this
          option is available your version of ADiMat was compiled with
          the DEBUG-symbol set.

    `--nobuiltins, [GLOBAL NOBUILTINS]'
          Skip any builtin-files. Even if there is a different
          builtinfilelist given, the builtin-files and '%ADiMat
          B*'-directives in the code are ignored.  Use this option with
          great care. The builtin-files define many Matlab-functions
          that are used often (see section *note Builtins::  for
          further information).

    `--builtinspath=<BUILDINSPATH>'
          This path is scanned for the builtins-declaration file
          'ADiMat.amb'. Specifing a new path discards the old one. Look
          at *note Builtins::

          for a more detailed description.
          (<BUILTINSPATH>='${ADIMAT_HOME}/share/adimat:~/.  adimat:.').

    `--exportctvcg=<FNAME>'
          Write the calltree of the functions to the file <FNAME>. The
          file will be written to the <OUTPUTDIR>. No absolute paths
          are possible, only paths relative to the <OUTPUTDIR>. The
          calltree is stored using the vcg format. The file gets the
          suffix '.vcg'. See section *note Calltree of functions::  for
          more information.

    `--exportctsimp=<FNAME>'
          Write the information gathered about caller-callees
          dependencies between function (the calltree) to the file
          <FNAME> in a simple text fashion. This file is intended for
          use with other programs, use --exportcttxt for a more human
          readable format. No suffix is appended to the filename. The
          file is put into the <OUTPUTDIR>. No absolute paths are
          allowed here. See section *note Calltree of functions::  for
          more information.

    `--exportcttxt=<FNAME>'
          Write the calltree to the file '<OUTPUTDIR>/<FNAME>' in a
          human readable format. No suffix is appended, but '.txt' is
          proposed here. See section *note Calltree of functions::  for
          more information.

    `--exportdgvcg=<FNAME>'
          The dependencies between all variables of the current project
          are written to the file '<OUTPUTDIR>/<FNAME>'. The
          filename-suffix '.vcg' is ensured. The variable dependency
          graph is only available in this format, because it is huge
          most of the time and other formats are not applicable. For
          documentation of the vcg format see section *note References::
          .

    `--nolocalmfiles, [GLOBAL NOLOCALMFILES]'
          Files that contain only statements and no function
          declarations are normaly copied into the scope they are
          called from and the file is renamed to meet the pattern:
          <calling_function_name>_<original_filename>. However, in some
          circumstances this may not be desired. Using this switch,
          will prevent the localization of these files. But make sure,
          that no activ variables are used or altered in the
          command-file. (The default is to use local files.)

    `--envoutput, [GLOBAL ENVOUTPUT]'
          This option writes more information about each identifier
          into the m-files written by ADiMat. The files are no longer
          Matlab-complient than. This option is for debuging purpose
          only. If this option is available your executable was
          compiled using the DEBUG-flag.

    `--gradprefix=<PREFIX>'
          This prefix is added to each active variable. The prefix is
          used for variables, when ADiMat is in forward mode.
          (<PREFIX>='g_')

    `--hessprefix=<PREFIX>'
          If the *note adimat Options::  flag is set, then this prefix
          is used to preceed the variables that store Hessians.
          (<PREFIX>='h_')

    `--funcprefix=<PREFIX>'
          The prefix is added to each function that is augemented in
          forward mode. There is no distinction between functions that
          are differentiated once or twice.  They all get this prefix.
          (<PREFIX>='g_')

    `-f, --forward-mode, [GLOBAL FORWARD-MODE]'
          Differentiate using the forward mode. For the reverse mode
          you have to use the command admrev.

    `-r, --reverse-mode, [GLOBAL REVERSE-MODE]'
          Differentiate the functions using the reverse mode. This mode
          is not available yet in the adimat binary. For the reverse
          mode source transformation you have to use the command admrev.

    `-m, --hybrid-mode, [GLOBAL HYBRID-MODE]'
          Differentiate the project using forward and reverse mode
          interchangeably. The decision when to switch modes is not yet
          defined. This flag is non-functional.  It is defined to
          prevent future misuse. The mode will be forced to forward
          even if this flag is used.

    `--outl_comp=<NUM>'
          Set the limit when a subexpression is truncated to the number
          given by <NUM>. See section *note Outlining - Simplifying
          complex expressions::  for further information. (<NUM>=1)

    `--nooperoptim, [GLOBAL NOOPEROPTIM]'
          During differentiation operations like a*b*c are joined to a
          single subroutine call times3(a, b, c) if a derivative object
          is involved. This flag prevents the optimization.

    `--nolocalcse, [GLOBAL NOLOCALCSE]'
          ADiMat splits up expressions that consist of more than one
          operation into many small expressions. Each expression is
          assigned to a temporary variable. While ADiMat operates on
          one such big expression it may encounter, that the same small
          expression is found several times. Instead of recomputing it
          each time, the expression is evaluated only once and the
          temporary variable is inserted multiple times into the
          expression. This switch turns this behaviour off. It may be
          of use if a function that manipulates a global variable is
          called multiple times.

    `--noglobalcse, [GLOBAL NOGLOBALCSE]'
          Similiar behaviour like *note adimat Options:: , but this
          time not only the expressions, which are generated during
          splitting the bigger expressions are taken into account.
          ADiMat will look at all expression in the program and
          replaces expressions which compute the same values with a
          temporary variable. The value of the temporary variable is
          then computed only once. This feature is switched off,
          whenever the flag --nolocalcse or its corresponding directive
          is used. Use this flag, if you use functions which have
          sideeffects and you encounter a wrong program behaviour after
          differentiation. !!! This flag is default currently due to a
          deficiency in the implementation. !!!

    `--globalcse, [GLOBAL GLOBALCSE]'
          Explicitly switch on global common subexpression elimination.
          Danger!!! The algorithm is buggy. Use at your own risk.

    `--noloopsaving, [GLOBAL NOLOOPSAVING]'
          ADiMat uses so called loopsaving operators when invoked
          without this flag. A loopsaving operator enables faster
          derivative computation, because several loops are joined
          togehter into one loop, therefore reducing the number of
          cycles needed. It is usually not a good idea to switch of the
          feature, because the differentiated code will perform badly
          then.

    `--nooverwrtchk, [GLOBAL NOOVERWRITINGCHECK]'
          Prevents "update" of variables. An expression `v=v*...;' is
          called an update of `v'. This is a bad idea, because `v' is
          most likely used in the derivative computation of `g_v=...'.
          The outcome of the expression will be assigned to a temporary
          variable. The temporary will be assigned to `v' in a
          successive statement. This switch turns off the check for
          variable updates. Again, be carefull if you use this switch.

    `--2ndorderfwd, [GLOBAL SECONDORDERFWDMD]'
          Insert statements to compute second order derivative using
          forward mode. In fact, the routines to compute first order
          derivatives are simply repeated once more.

    `--parentcheck, [GLOBAL PARENTCHECK]'
          Switch on parent checking in AST. This is flag is available
          only, if ADiMat was compiled with DEBUG-support. The check is
          done directly after all files are read.

    `--canonparentcheck, [GLOBAL CANON_PARENTCHECK]'
          Check that the AST is correctly linked. The check is applied
          after code canonicalization. Debugging.

    `--postparentcheck, [GLOBAL POST_PARENTCHECK]'
          Check correct linking of the AST after augmentation and
          shortly before output.  Debugging.

    `--allparentchecks'
          Use all of the above checks. This flag is available only, if
          ADiMat was compiled in DEBUG mode.

    `--noscalarfolding, [GLOBAL NOSCALARFOLDING]'
          Suppress folding of constant real scalar expressions.
          Scalarfolding examines expressions and computes the result of
          constant expressions, like `3+4'.  This option disables all
          folding of scalar values.

    `--noprescalarfolding, [GLOBAL NOPRESCALARFOLDING]'
          Suppress scalar folding before the augmentation step.

    `--nopostscalarfolding, [GLOBAL NOPOSTSCALARFOLDING]'
          Suppress scalar folding after the augmentation step. It is
          not advisable to switch of scalarfolding, because ADiMat will
          generate some constant expressions, e.g. `2*x^(2-1)*g_x' for
          the derivative of `x^2', where a constant term could be
          eliminated. Use this switch if you believe, that ADiMat does
          incorrect folding.

    `--noconstfoldmult0, [GLOBAL NOCONSTFOLDMULT0]'
          Suppress folding of expressions with one factor being zero.
          ADiMat usually folds products of the kind `bar= foo*0' to
          `bar= 0'. In some special, and seldom cases this may be
          undesireable. E.g. if foo is a function call having side
          effects.

    `--nonargsupport, [GLOBAL NONARGINMAPPER], [GLOBAL'
          NONARGOUTMAPPER]

          Suppress the generation of mapping arrays for nargin and
          nargout identifiers.  In Matlab a function can check how many
          arguments are set by the calling context by checking the
          nargin identifier. During augmentation the parameter list is
          changed. The mapping arrays take care about mapping the
          number of arguments correctly to the number of arguments the
          code expects without the derivative objects in the parameter
          list. Usually there is no need to deactivate generation of
          narg-mapper.

    `--notmpclear, [GLOBAL NOTMPCLEAR]'
          ADiMat generates `clear' instructions for temporary
          variables, that are inserted by ADiMat itself. This conserves
          memory and speeds up computation, because Matlab's workspace
          size is smaller. It is usually a bad idea to switch off the
          generation of these clear commands, but it may be necessary,
          if temporary variables are cleared, that ought to exist
          longer. Normally this occurs only, if some tricky programming
          is used. Note, these clear commands only free memory of
          temporary variables introduced by ADiMat.

    `--visitfwdmd, [GLOBAL VISITFWDMD]'
          This is a new implementation of the forward mode
          differentation approach of AD. It is currently in beta state,
          but enables the use of more complex source code analysis and
          application of replace actions for toolbox-functions. This is
          the default. The flag is present for backward compatibilty
          and will be deleted in future versions.

    `--novisitfwdmd, [GLOBAL NOVISITFWDMD]'
          Switch back to old AST-driven differentiation of AD. Beware,
          this mode is not able to different every Matlab-program. It
          is not advised to switch off visitfwdmd. If you do not know
          what we are talking about, then do not use this flag.

    `--no0pow0check, [GLOBAL NOZEROPOWZEROCHECK]'
          If Matlab has to compute x^y where x and y are zero, then the
          result is set to 1. This behaviour is choosen by Matlab. To
          mimick this behaviour correctly, ADiMat has to check if 0^0
          operations occur and set the derivative at these entries to
          0.0. This flag prevents ADiMat from inserting the code
          executing these checks. If you know, that your code never
          computes 0^0, then you may switch off generation of this
          code. But note, that the derivative code may possibly compute
          incorrect derivatives (NaN values!).  If you get NaN's and
          have used this flag, then try again without the flag.

    `--nosigensure, [GLOBAL NOSIGNATUREENSURE]'
          If functions are called by distinct functions using varying
          parameter lists, then ADiMat computes the transitive closure
          of all variables that need derivative objects associated to
          it. For most functions the sets of variables that need
          derivative objects are equal, but in some cases the sets
          differ and dummy derivative objects and results have to be
          inserted to call the function correctly. This switch prevents
          ADiMat from inserting these dummy derivative objects and the
          dummy results.



File: adimat.info,  Node: Directives,  Next: Environment variables,  Prev: Usage of classic adimat,  Up: Top

10 Directives
*************

ADiMat uses command line options or directives within the parsed
sources to configure its behavior. In section *note Usage::

   options are described in greater detail.

* Menu:

* adimat Directives::
* admproc Directives::


File: adimat.info,  Node: adimat Directives,  Next: admproc Directives,  Up: Directives

10.1 adimat Directives
======================

Directives are Matlab-comments starting with the keyword 'ADiMat'. A
directive is used to manipulate the behaviour of ADiMat during its run.
Nearly all directives may appear anywhere in the code. The expections
from this rule are the directives starting with the prefix 'AD_'. They
have to appear in the first input file given to ADiMat. An occurrence
in any additional file is not forbidden, but does not make any sense.
Directives used to switch a specific feature on or off are named
together, seperating them using a commata. Some features of ADiMat may
be selected using a command line option or a directive. The directive
overrides the command line option in every case. The corresponding
command line option is named with its directive, respectively. The
directives available are:


    `AD_TOP=<FUNCNAME>'
          The name of the function to differentiate. The function has
          to be called in the current file using the variables defined
          as independent or dependent ones using the AD_IVARS- and
          AD_DVARS-directives, respectively.

    `AD_IVARS=<IVARLIST>'
          Specifies the list of independent variables. The variables
          with which a function is differentiated with respect to are
          called independent variables.  Multiple variables may be
          named here using a commata ',' to seperate them.

    `AD_IVAR=<IVAR>'
          A synonym for the above directive which can be used in the
          case of a single variable being specified. ADiMat will also
          accept a list though, it does not engange in english grammar
          checking.

    `AD_INDEPENDENT=<IVARLIST>'
          A synonym for the two above directives.

    `AD_DVARS=<DVARLIST>'
          Specifies the list of output variables of the top-level
          function for which derivatives are sought. More than one
          variable may be named by seperating each using a commata ','.

    `AD_DVAR=<DVAR>'
          A synonym for the above directive which can be used in the
          case of a single variable being specified. ADiMat will also
          accept a list though, it does not engange in english grammar
          checking.

    `AD_DEPENDENT=<DVARLIST>'
          A synonym for the two above directives.

    `AD_OUTPUT_DIR="<OUTPUTDIR>"'
          All files that are written by ADiMat will be created in the
          directory given by <OUTPUTDIR>. Please note the double quotes
          around the <OUTPUTDIR>.  These are mandatory, because without
          them a slash within the directory would be considered to be
          the division-operator. The directive is only usefull in the
          first file of the current project to differentiate. It is
          optional, but overrides the '-p' or '--outputdir' flag of the
          commandline.

    `GLOBAL FORWARDMODE, GLOBAL FORWARD-MODE'
          Select the forward mode of automatic differentiation.
          Currently this flag has a global scope, that is, specifying
          it anywhere will switch the mode of differentitation of the
          current project to forward mode. Until now no alternative
          modes are implemented, the directive may be obmitted
          therefore.  But it is considered good style to specify it, if
          a project is to be differentiated using forward mode anytime.
          The command line switches overriden by this directive are:
          '-f', '--forward-mode', '-r', '--reverse-mode', '-m' and
          '--hybrid-mode'.

    `GLOBAL REVERSEMODE, GLOBAL REVERSE-MODE'
          This directive is non-functional currently. The reverse mode
          of automatic differentiation is not implemented yet.
          Specifying this flag, yields a warning, but ADiMat continues
          to differentiate the current project using the forward
          mode.The command line switches overriden by this directive
          are: '-f', '--forward-mode', '-r', '--reverse-mode', '-m' and
          '--hybrid-mode'.

    `GLOBAL HYBRIDMODE, GLOBAL HYBRID-MODE'
          Same like reverse mode. Not implemented yet. Specifying it,
          results in a warning. The command line switches overriden by
          this directive are: '-f', '--forward-mode', '-r',
          '--reverse-mode', '-m' and '--hybrid-mode'.

    `GLOBAL NOFORWARDVARRESOLVE, GLOBAL FORWARDVARRESOLVE'
          This directive is implemented for highly sophisticated code
          (also called `bad style code'). In Matlab it is possible to
          use an identifier as a function and later on to denote a
          variable. This is considered bad style, but supported by
          ADiMat. The standard behaviour of ADiMat is to look forward
          in the parsed code to find a defining occurrence of an
          identifier (that is, a left-hand side occurrence of a
          variable). That is, in the example 'b=a; a=3;' ADiMat will
          treat the first 'a' like the second one, allthough 'a' was
          not defined before first use. A warning will be issued. Note,
          it may not be an error that a variable occurs in a using
          context before it is defined, think about an 'if then else'-
          construct, where the variable is defined in the else part and
          used only in the then part. For common codes the directive
          'GLOBAL FORWARDVARRESOLVE' is suitable, which is the default.
          If a code fails to be parsed, because of problems during the
          identification of the identifiers, try this directive. This
          directive overrides the command line option
          '--noforwardvarresolve'.

    `GLOBAL WRITESCOPEFILE'
          Write the file '<OUTPUTDIR>/adimat.dbg' which contains
          debuging information about the binding of identifiers. This
          directive is for debugging use only.

    `GLOBAL NOBUILTINS'
          Switch off the builtin system. This is no good idea!
          Specifying this option hinders ADiMat's use of derivative
          information stored in the database. None of the hunderts of
          builtin Matlab-functions will get differentiated, nor is it
          possible to specify *note BMFUNC::  directives.

    `GLOBAL NOLOCALMFILES, GLOBAL LOCALMFILES'
          Matlab allows two sorts of files. Files containing a function
          and files containing statements only. The former kind is not
          affected by this directive, but the later. A statement file
          (called m-file here) causes a problem, when called from more
          than one function. Different sets of variables may be active.
          If the directive 'LOCALMFILES' is used, which is the default,
          each m-file will be duplicated each time it is called from a
          function and the name of the calling function is concatenated
          to the m-file name. This localises the m-file and enables
          correct identification of the identifiers within the file.
          This techinque is in alpha-state and therefore not applicable
          every time. If ADiMat complains about missing identifiers and
          the project contains an m-file, then the directive 'GLOBAL
          NOLOCALMFILES' may solve the problem. This directive
          overrides the command line option '--nolocalmfiles'.

    `GLOBAL ENVOUTPUT'
          Each identifier in the whole program is augmented with
          information of its state within ADiMat. The output-files
          produced by ADiMat are no longer valid Matlab-files. The
          directive is for debuging purpose only.

    `GLOBAL NOOPEROPTIM, GLOBAL OPEROPTIM'
          These directives disable/ enable the use of an optimisation,
          if a binary operator is used more than once successively to
          compute a derivative. Assume the expression a* b* c is to be
          differentiated with respect to all three variables. If the
          flag NOOPEROPTIM is set, then the derivative looks like this:
          g_a* b* c+ a* g_b* c+ a* b *g_c. This code is not very
          efficient, because each multiplication of a derivative object
          g_? with any over object executes a loop for each operator.
          That is, for the example above 7 loops are executed (the
          product of a* b contains no derivative object and no loop
          therefore). Using the flag OPEROPTIM the above derivative
          expression will be replaced with a function call and the
          number of loops executed will be decreased significantly
          (currently five loops are executed, but in future versions
          only one loop may be executed). The default is to use
          OPEROPTIM, the ability to switch of the optimization of
          operator differentiation is merely a tool for
          performance-meassurement. This directive overrides the
          command line option '--nooperoptim'.

    `GLOBAL NOLOCALCSE, GLOBAL LOCALCSE'
          Dis-/Enables local common subexpression elimination (cse).
          The transformation is applied to all files if this statement
          is encountered anywhere. During the canonicalisation step
          ADiMat may encounter that one subexpression is evaluated
          several times. The code is transformed in a way that the
          subexpression is evaluated only once and stored in a
          temporary variable. The temporary variable is inserted into
          the code where a reevaluation of the subexpression would have
          occurred. This directive switches off/on the optimization
          strategy. This may be needed, if one of the subexpressions
          contained a function with sideeffects.  This directive
          implies setting the *note adimat Directives::  directive. The
          directive overrides '--nolocalcse', '--localcse' and
          partially '--globalcse'. The latter one is only partially
          overriden, because switching off local cse switches off
          global cse, too.

    `GLOBAL NOGLOBALCSE, GLOBAL GLOBALCSE'
          Switches off/on global common subexpression elimination.
          ADiMat applies a common subexpression elimination algorithm
          to the whole code after the canonicalization step. This
          directive may be of use, if the code contains functions with
          side effects. The resulting code may be incorrect. Try this
          directive or the commandline option *note adimat Options::
          to check if the cse caused the problem. This directive
          overrides '--noglobalcse', '--globalcse' and partially
          '--localcse'. The latter one is overriden partially, because
          global cse needs local cse.

    `GLOBAL NOLOOPSAVING, GLOBAL LOOPSAVING'
          Disables/Enables optimized operators in derivative
          computations. During computation of derivatives loops iterate
          over the number of derivatives. In the derivative of a
          product three loops are run. The loopsaving operator, in fact
          a function, combines these three loops to one single loop
          minimizing the computational effort. It is highly recommended
          to use loopsaving operators/functions. The loopsaving
          functions are denote by the 'ls_'-prefix.  This directive
          overrides the '--noloopsaving' flag.

    `GLOBAL NOOVERWRITINGCHECK, GLOBAL OVERWRITINGCHECK'
          Disable/Enable the check for variable updates. A variable
          update is an assignment to a variable `v' where the variable
          occurs on the right- and left-hand side of the assignment. In
          fact, it is not only checked if an update occured, but it is
          also prevented that the variable `v' is overwritten in the
          same assignment it is used in. The motivation for this check
          is simply the fact, that within the derivative expression of
          the assignment, the original value of `v' before the update
          may be needed. This directive overrides the option
          '--nooverwrtchk'.

    `GLOBAL SECONDORDERFWD, GLOBAL SECONDORDERFWDMD'
          Compute Hessians along with the gradients. This directive
          switches on the computation of second-order derivatives.

    `GLOBAL PARENTCHECK, GLOBAL CANON_PARENTCHECK, GLOBAL POST_PARENTCHECK'
          Check consistency of the AST. This is a debugging directive.
          See *note adimat Options::  for more information.

    `GLOBAL NOSCALARFOLDING, GLOBAL SCALARFOLDING, GLOBAL NOPRESCALARFOLDING,'
          GLOBAL PRESCALARFOLDING, GLOBAL NOPOSTSCALARFOLDING, GLOBAL
          POSTSCALARFOLDING Disable/Enable folding of constant real
          scalar expressions at transformation time. Have a look at the
          commandline option for more information (see *note adimat
          Options:: ). These directives override: '--noscalarfolding',
          '--noprescalarfolding', and '--nopostscalarfolding'.

    `GLOBAL NOCONSTFOLDMULT0, GLOBAL CONSTFOLDMULT0'
          If *note adimat Options::  is enabled, then this directive
          can be used to switch off and on the folding of products
          whose one factor is zero. This directive overrides:
          '--noconstfoldmult0'.

    `GLOBAL NONARGINMAPPER, GLOBAL NARGINMAPPER'
          Omit/permit creation of nargin mappers for functions using
          the Matlab function nargin. See command line option *note
          adimat Options::  for more information. This option allows a
          finer control over the created mappers, by permitting
          creation for nargin only.

    `GLOBAL NONARGOUTMAPPER, GLOBAL NARGOUTMAPPER'
          Omit/permit creation of nargout mappers for functions using
          the Matlab function nargout. See command line option *note
          adimat Options::  for more information. This option allows a
          finer control over the created mappers, by permitting
          creation for nargout only.

    `GLOBAL NOTMPCLEAR, GLOBAL TMPCLEAR'
          Omit/Insert statements for clearing temporary variables
          inserted by ADiMat. See command line option *note adimat
          Options::  for more information.

    `GLOBAL NOVISITFWDMD, GLOBAL VISITFWDMD'
          Use the old way of computing forward mode derivatives. Beware
          the old way is not capable of treating all Matlab constructs
          correctly. It is strongly advised not to use these options.
          The associated command line switch is *note adimat Options:: .

    `GLOBAL NOZEROPOWZEROCHECK, GLOBAL ZEROPOWZEROCHECK'
          Omit/permit creation of code for computing the correct
          derivative for 0^0 statements. See *note adimat Options::
          for more information.

    `GLOBAL NOSIGNATUREENSURE, GLOBAL SIGNATUREENSURE'
          Prevent/Force ADiMat from/to insert dummy objects for
          function's parameter/result lists that are incompatibly
          called from more than two functions and where the transitive
          closure recommends a certain signature. See *note adimat
          Options::  for more information.

    `BMFUNC ..., BJFUNC ..., BCFUNC ..., BCOMMAND ..., BVAR ...'
          The builtin-declarations may be used like a directive, too.
          For their description look at the chapter *note Builtins:: .

    `FUNCTION_HANDLE=<VARLIST>'
          Specifies a list of variables that have class function_handle
          in the current scope. ADiMat needs this declaration to
          distinguish between array indexing and calls of function
          handles, as both are syntactically equal. ADiMat will treat
          the calls to declared function handles similarly as calls to
          user defined functions. This means that for any function
          handle foo, function handles g_foo (FM), rec_foo, and ret_foo
          (RM) must also be available. If a function parameter is
          declared to be of type function_handle, ADiMat will
          automatically augment the signature to contain these names as
          well.  Consider the function signature

               function r = someFunc(a,b,f_ptr)

          Inside the function body the directive FUNCTION_HANDLE must
          be used:

               %ADiMat FUNCTION_HANDLE=f_ptr

          If all parameters are active, in FM the signature will be
          augmented as follows

               function [g_r, r] = g_someFunc(g_a,a, g_b,b, g_f_ptr,f_ptr)

          In a driver routine, g_someFunc should be called something
          like that

               [g_a g_b] = createFullGradients(a,b);
               [g_r r] = g_someFunc(g_a,a, g_b,b, @g_otherFunc,@otherFunc)

          In RM the signature will be augmented as follows

               function [a_a, a_b, nr_r] = a_someFunc(a,b,f_ptr,rec_ptr,ret_ptr)

          In a driver routine, a_someFunc should be called something
          like that

               r = someFunc(a,b,@otherFunc)
               [a_r] = createFullGradients(r);
               [a_a a_b r] = a_someFunc(a, b, @otherFunc, @rec_otherFunc, @ret_otherFunc)

          The top level derivative functions must by called with
          additional arguments representing function handles to the
          differentiated versions of function otherfunc. The function
          otherfunc should be differentiated with ADiMat in a separate
          step.  There are some limitations of this directive:

             * Function handles must be declared separately in each
               function where they occur in.

             * The function the handle refers to will not be
               differentiated automatically.

             * All arguments of calls to functions handles will be
               differentiated.

             * The activity analysis can usually detect whether a
               function handle variable is active and will not
               differentiate it if that is not necessary. However, when
               a function handle is not used inside a function f, but
               only passed on to a subfunction g, then it may be
               necessary to declare the function handle variable as
               active using AD_ACTIVE=f_ptr to produce the correct
               calls to g and the augmented signature in f.

    `AD_ACTIVE=<VARLIST>'
          Specifies a list of variables that ADiMat should treat as
          being active in the current scope (i.e. where the directive
          occures). Normally any variable on a path from a independent
          to a dependent variable is active and this will be detected
          by ADiMat's activiy analysis. This directive only serves as
          an escape hatch should that fail somehow. There are currently
          two possible scenarios where this directive is useful:

             * Handling function handles that are not called in a
               certain function but passed on as arguments to
               subfunction calls.

             * Forcing ADiMat to produce derivative parameters for some
               parameters that are not active. This can be used to
               force ADiMat to produce a certain function signature.


   The next few lines show an example using the most commonly used
directives within the file given to ADiMat at the command line.

     % First the mandatory data
     %ADiMat AD_DVARS= z
     %ADiMat AD_IVARS= b
     %ADiMat AD_TOP= f
     % This is optional
     %ADiMat AD_OUTPUT_DIR="ad_out/"
     b=0.1:0.1:100;
     z=f(b);

   NOTE! Each ADiMat-directive within a Matlab file has to be preceeded
by the string '%ADiMat ' exactly as written here. No space after the
percent-sign is allowed nor different combinations of upper- and
lowercase letters in the word ADiMat are allowed. Using this strict
rule a directive may be switched of by simply altering the case of one
letter or inserting a space after the percent-sign.


File: adimat.info,  Node: admproc Directives,  Prev: adimat Directives,  Up: Directives

10.2 admproc Directives
=======================

As with adimat, admrev directives are Matlab-comments starting with the
keyword 'ADiMat'. The remainder of the line must be a well-formed XML
subtree, however.

     function foo = bar(x,y,z)
     %
     %ADiMat <nargin>3</nargin>
       foo = x*y+z;

   There are not many directives yet.


    `<parallel/>'
          Declare that the for loop in which the directive occurs is
          parallelizable. This is the case if the order of execution of
          the loop body does not matter.  The loop iterations in the
          forward sweep will not be recorded, instead the loop is
          differentiated by performing little forward and return sweeps
          on the loop body during the function's return sweep.

    `<nargin>number</nargin>'
          Declare that the function in which the directive occurs will
          always be called with at least number arguments. This helps
          ADiMat to decide which of the functions arguments are to be
          saved for the return sweep.

    `<recompute>name</recompute>'
          Declare that the call(s) to function name in the following
          statement is to be recomputed in the reverse sweep. This has
          the effect that function name is called in the storage sweep
          and function a_name in the reverse sweep. The normal strategy
          is to save the computattions of the storage sweep, calling
          rec_name in the storage and ret_name in the reverse sweep.



File: adimat.info,  Node: Environment variables,  Next: Dependecy graphs and further information,  Prev: Directives,  Up: Top

11 Environment variables
************************

ADiMat uses some enviromnent variables to customize its behaviour. The
directory where the additional data is installed may be modified using
the ADIMAT_HOME environment variable, for example. If the variables
introduced here are not set on the machine, then default values will be
used.

   Currently these environment variables are used to customize the
behaviour of ADiMat:

`${ADIMAT_HOME}'
     Specify the path where the ADiMat is installed. The default value
     is: '/usr/local' on Linux and Unix and 'c:/adimat' on Windows. In
     particular the binary programs reside in '${ADIMAT_HOME}/bin'.

`${ADIMAT_USER_HOME}'
     Specify the path user information is found. The default is
     '${HOME}/.adimat/' on Linux and Unix. On Windows, the default is
     '<appdata>/ADiMat/', where <appdata> is determined as follows:
     First the APPDATA folder is searched through the Windows API, then
     '${APPDATA}' is evaluated, and finally '${HOMEDRIVE}${HOMEPATH}',
     which defaults to 'c:\'

`${ADIMAT_BUILTINSPATH}'
     ADiMat looks up the file 'ADiMat.amdb' in this path. The first
     occurrence , of this file is used (see section *note Builtins::
     for further information). The default path is:
     '${ADIMAT_HOME}/share/adimat:${HOME}/.adimat:.  ' on Unix systems
     and '${ADIMAT_HOME}/builtins' on Windows systems.

`${ADIMAT_DERIV_CLASSPATH}'

`${ADIMAT_DERIVCLASS}'
     The ADiMat_startup script provided with this distribution uses this
     variable to set the derivative class. Note you can also use the
     `adimat_derivclass' function after Matlab has started (see *note
     Derivative and adjoint classes:: ).

`${ADIMAT_ADJOINT}'
     The ADiMat_startup script provided with this distribution uses this
     variable to set the type of adjoints used. Note you can also use
     the `adimat_adjoint' function after Matlab has started (see *note
     Derivative and adjoint classes:: ).

`${ADIMAT_STACK}'
     The ADiMat_startup script provided with this distribution uses this
     variable to set the stack implementation used. Note you can also
     use the `adimat_stack' function after Matlab has started (see
     *note Stacks for the reverse mode:: ).

`${ADIMAT_STACK_FILE}'
     This variable is used by some stack implementations as the name of
     the file to save values that are pushed on the stack. If this
     variables is not set, then that file is created in the directory
     named by ${TMP}.

`${ADMTRANSFORM_FLAGS}'
     This variable is read by admTransform and can be used to add flags
     to the source transformation command. The default is empty. You
     can also use the field flags of admOptions. See *note Interfaces
     to program transformation::  and *note High level user interface
     functions:: .

`${ADMTRANSFORM_PROGRAM}'
     This variable is read by admTransform and can be used to set the
     name of the source transformation program. The default is
     ${ADIMAT_HOME}/bin/adimat-client. See *note Interfaces to program
     transformation:: .

`${ADMTRANSFORM_WRAP}'
     This variable is read by admTransform and can be used to set the
     name of the wrapper skript for the source transformation command.
     On Unix, the default is `${ADIMAT_HOME}'bin/admwrap -K
     <matlabroot>/, where <matlabroot> is the result of the command
     matlabroot. This wrapper script removes all directories under
     matlabroot from LD_LIBRARY_PATH. If this is not done, adimat-client
     find shared libraries from there, which are incompatible. On PCs
     (Windows) the default is empty. See *note Interfaces to program
     transformation:: .


   Any other environment-variable that is named in a path or likewise,
will be replaced by the value the operating system returns when asked
for it.


File: adimat.info,  Node: Dependecy graphs and further information,  Next: Outlining - Simplifying complex expressions,  Prev: Environment variables,  Up: Top

12 Dependecy graphs and further information
*******************************************

ADiMat gathers information about the Matlab project analysed the user
may be interested in. This information may be added to the output files
or written to distinct files, which may be analysed using third party
software.

* Menu:

* Calltree of functions::
* Variable dependency graph::
* Debugging information::


File: adimat.info,  Node: Calltree of functions,  Next: Variable dependency graph,  Up: Dependecy graphs and further information

12.1 Calltree of functions
==========================

The calltree shows dependencies between functions. The relation which
function is called by which and which function calls which is
generated. This information may be written to a file using different
output-formats. The most convenient is the *note References:: . This
graph description language is parsed by a visualizer and may be browsed
by the user. The background color of an active function is changed to
green. The visualizer may be obtained using the url in the this
bibliography-entry *note References:: . The flag *note adimat Options::
is used to store the calltree in vcg-format.  Furthermore, one human
readable format is available. The calltree is written in simple ASCII.
The file contains the list of all functions and the names of the
functions, that are called by this function. A function name 'f' may
occur more than once in the list functions called by a function 'g',
which indicates that the function 'f' is called more than once in the
function body of 'g'. Subfunctions local to a specific function are
written like being top-level functions. The output may be generated
using the option *note adimat Options:: . This format is implemented
only, if no vcg-tool is availabe.  The last available output-format
writes the calltree in a machine readable format. All functions get a
unique number and a list of callees and callers.  Both list enlist the
numbers of the functions that are called by the current function (the
callees-list) and that call the current function. A single number may
occur more than once in a list, which shows that the function calls or
is called more than once by this function. The file may be generated
using the option *note adimat Options:: .


File: adimat.info,  Node: Variable dependency graph,  Next: Debugging information,  Prev: Calltree of functions,  Up: Dependecy graphs and further information

12.2 Variable dependency graph
==============================

ADiMat may write a file containing the dependencies between variables. A
variable 'u' depends on a variable 'v', if the variable 'v' occurs on
the right-hand side of an assignment to 'u'. The only available
output-format is the vcg-format. The dependencies are shown as a graph,
where the variables are the nodes and the dependencies are the edges.
The edges are directed in the way, that the tip points to the variable
that is on the right-hand side of the assignment. Each variable is
displayed using a box. The variable-name is prefixed using the
scope-name in which the variable is defined or used. Global variables
are marked by using a rhombus instead of a box. The background color of
the boxes or rhombuses indicate the activation state of the variable.
Four states and therefore colors are possible. A white background
indicates that the variable is inactive, a green background that the
variable is active, a red background that the variable is reachable
from an independent variable and a yellow background denotes that the
variable is reachable from an dependent variable.  Use the option *note
adimat Options::  to get the file in *note References:: .


File: adimat.info,  Node: Debugging information,  Prev: Variable dependency graph,  Up: Dependecy graphs and further information

12.3 Debugging information
==========================

Two additional options exist, that enable the debuging of an ADiMat
run. Both deliver information about the binding of identifiers. The
binding of an identifier to a specific environment and scope is
described.  The first option *note adimat Options::  generates the file
'<OUTPUT_DIR>/adimat.dbg'. The file contains a trace of the process of
identifying identifiers, called identifier-binding. The steps needed to
bind all identifiers are enlisted. In each step the scopes and their
environments are printed. Every function creates its own scope. Every
scope has three environments. One for identifiers that represent
variable-names, one for function-names and one environment that stores
identifiers, whose use is not yet encountered. In each environment the
identifiers allready identified are enlisted. Each identifier shows its
occurences in the code and some further information. The traces enables
to comprehend the process and find errors. This file is used identify
errors during the development of ADiMat.  Tool-developers may have an
interest in this file.  The second option *note adimat Options::  is
hazardous in a certain point of view. If the option is used, the
written Matlab-files are augmented with additional information for each
identifier. After each identifier in all Matlab-.m-files the
information in which scope and environment this specific identifier is
bound. The file is no longer understandable by Matlab. The information
added by this flag has debugging purpose only. The normal user does not
need to bother about it.


File: adimat.info,  Node: Outlining - Simplifying complex expressions,  Next: Builtins,  Prev: Dependecy graphs and further information,  Up: Top

13 Outlining - Simplifying complex expressions
**********************************************

Matlab enables the use of rather complex formula. These expressions are
unwieldy. ADiMat may break up complex active expressions into many
smaller expressions, this process is called 'outlining' in contrast to
the inlining process performed by many compilers. The 'outlined'
subexpressions are assigned to uniquely generated temporary variables.
The temporary variables are inserted into the complex expression where
the subexpression was removed.  This process has some advantages:

  1.  during the computation of the derivatives the differentiated
     expressions are shorter,

  2.  the differentiated code is easier to check, and

  3.  the time to compute the derivative of an expression is reduced,
     because instead of differentiating an expression a variable is
     used.

   The decision where to take a subexpression out of the whole
expression is made by rating each item (variable, operator, ...) of the
expression using a cost function. If the costs of a subexpression
exceed a user-defineable limit, the subexpression is outlined. The
costs of a subexpression are defined by the table given below. The
costs of each item where choosen with respect to the expected
complexity of the derivative expression. The costs do not reflect a
meassured execution time or likewise. The symbols used in the table are:

`$c$'
     for constant expressions, like numbers or operations that do not
     contain an active variable,

`$x$'
     for an active variable,

`$u,v,w$'
     for subexpressions that may contain active variables,

`$f()$'
     for calls to user-defined functions,

`$bui()$'
     for calls to functions that are defined to be built into Matlab,

`$u_(c),v_(c),w_(c)$'
     for the costs of the corresponding subexpression,

`_infinity_'
     telling that the coresponding expression is outlined where ever it
     occurs (See below for more information), and

`$SUM(i=1..n,expr)$'
     for the sum of n summands of the expressions $expr$ using $i$ for
     indexing.



     operation   |  cost
     $v = c$  | $v_(c) = 0$
     $v = x$  | $v_(c) = 1$
     $v = u +/- w$  | $v_(c) = u_(c) + w_(c)$
     $v = u * w$  | $v_(c) = 2 * ( u_(c) + w_(c))$
     | $v_(c) = 4 * ( u_(c) + w_(c) )$
     $v = u^(c)$  | $v_(c) = 3 * u_(c)$
     $v = u \ w$  | $v_(c) = $_infinity_
     $v= f(u_(1), ..., u_(n))$  | $v_(c) = $_infinity_
     $v = bui(u_(1), ..., u_(n))$  | $v_(c) = SUM(i=1..n,u_(c,i))/n$

   Two expressions are rated to have infinite costs, because they have
to be outlined whereever they occur. The backslash operator has to be
outlined whereever it occurs, because the derivative expression uses
the result of the original expression $y=A$\$d$. The computation of $y$
is expensive, because the backslash-operator solves the linear equation
$A*y=d$. Reusing the result is recommended therefore. The second
expression that has to outlined whereever it occurs is the call of a
user-defined function. The differentiated version of a user-defined
function returns the double number of active results. A function that
returns only one result in the original version, returns the original
result and a derivative object in the differentiated version. To get
both results of the differentiated version the function has to be
called using an expression like $[gy, y]= gf(...)$.


File: adimat.info,  Node: Builtins,  Next: ToDo,  Prev: Outlining - Simplifying complex expressions,  Up: Top

14 Builtins
***********

Builtin-declarations are 'equations' that define a function of Matlab
and its differentiated function or code. Matlab has a set of builtin-,
intrinsic- or toolbox-supplied-functions (for simplicity these
functions are called toolbox-functions from now on), that are not
defined in a .m-file (allthough one may exists!), but are known to the
MatLab-executable. The code of these functions can not be
differentiated by ADiMat, because the .m-file of these functions only
contains a description (some comments) and no code.  Furthermore the
differentiation of such a function is done by replacing the function
call using a different function (e.g. The derivative of $sin(x)$ is
$cos(x)$). Another way had to be choosen to specify the derivative of
these functions: builtin-declarations.

   A builtin-declaration specifies the signature of a function, (e.g.
the signature of the sinus is: $$$=sin($1)$) and the corresponding
actions of ADiMat when differentiating this function. Possible actions
are '`DIFFTO" (may also be written '`DIFFSTO"), '`REPLACE" (also
'`REPL"), '`IGNORE", '`NODIFF", '`ERROR", '`WARNING", and '`SPECIAL"
(also '`SPEC"). With these actions it is possible to specify the
differentiated code of a function, replace a function call to compute
its original result and derivative, ignore the function when
differentiating it, do not differentiate this function, print an
error-message, add a warning message where ever the specified function
occurs, and do something not yet known to ADiMat in the current
version, respectively. The last modifier is reserved for extensions in
future versions of ADiMat. The following sections briefly discuss the
structure of the file where the builtin-declarations are specified in
and the builtin-declarations themselves.

   Starting with version 0.5 ADiMat no longer uses the .amb-files
directly, but uses a preprocessed form of the builtin-declarations
stored in an SQL-database.  Nevertheless the .amb-files are used to
build the database using the tool *note The dbbuild tool:: . The
builtin-declarations known to ADiMat prior to version 0.5 may also be
used as directives in Matlab-files to differentiate (see *note adimat
Directives:: ).

* Menu:

* -amb-files::
* Builtin-declarations::
* funcsig::
* actionstring::
* types::
* BVAR::
* BMFUNC::
* BCFUNC::
* BJFUNC::
* The dbbuild tool::


File: adimat.info,  Node: -amb-files,  Next: Builtin-declarations,  Up: Builtins

14.1 -amb-files
===============

The postfix '.amb' is the short form of *A*Di*M*at*B*uiltins. A
.amb-file may contain zero (does not really make sense) or more
builtin-declarations. It also may contain empty lines and comments,
which will be ignored. A comment starts with a '%' and runs to the end
of the line. !!! A comment starting with '%ADiMat' is not ignored, it
is treated as a regular builtin-declaration and has to conform to the
specification of a builtin-declaration. Whenever an error is encountred
while parsing a .amb-file a message displayed and the line is ignored.
The parsing of the file continues at the next line.  The *note The
dbbuild tool:: 's default is to look for the .amb-file 'ADiMat.amb' in
the following path (in this order):

   * ${ADIMAT_HOME}/share/adimat (on Unix)

   * ${ADIMAT_HOME}/builtins (on Windows)

   * ${HOME}/.adimat (the directory .adimat in the users home directory)

   * . (the current directory)

   The lookup-process terminates as soon as the file is found. The user
may specify his own .amb-path-list by using the command line option
*note The dbbuild tool::  or the environment-variable *note Environment
variables:: . Use of environment-variables within the .amb-path-list is
permitted. ADiMat will try to fetch the values from the
machine-environment and replace the variable-names in the path-list
with their values (if not allready done by the shell). If a directory
is not found or is unreadable, a message is printed and the according
directory is skipped.  One special .amb-file exists. It defines the
symbols used within the Matlab-class of ADiMat. The file is named
'ADiMat_internal.amb'. Its content is essential for a correct
differentiation of most Matlab-files. In the current implementation the
file is included by 'ADiMat.amb'.


File: adimat.info,  Node: Builtin-declarations,  Next: funcsig,  Prev: -amb-files,  Up: Builtins

14.2 Builtin-declarations
=========================

This section describes the syntax and keywords of builtin-declarations.
First some explanations of the language used to define the syntax:
Keywords and literals are written in single quotes and have to appear
in the declaration without the quotes. Variable- and function-names are
written: <name> and have to conform with the MatLab-identifier rules.
They may begin with a character (upper- and lower-case) or an
underscore followed by one or more (upper- or lower-case) characters,
underscores and numbers in any order.  Unquoted lowercase words
represent rules that will be expanded to their contents.  <NUM>
represents a non-fractional number starting with one.


File: adimat.info,  Node: funcsig,  Next: actionstring,  Prev: Builtin-declarations,  Up: Builtins

14.3 funcsig
============

`Description:'
     Describe the signature of a function.

`Syntax:'
          `<funcname>

          <funcname>'('')'

          <funcname>'('parameters')'

          '$$''='<funcname>

          '$$''='<funcname>'('')'

          '$$''='<funcname>'('parameters')'

          '['returns']='<funcname>

          '['returns']='<funcname>'('')'

          '['returns']='<funcname>'('parameters')'
          '

`Semantic:'
     A function signature specifies the name of the function and the
     number of its return arguments and parameters. Optionally a type
     may be specified with each return argument and parameter.  A
     function name may be followed by an optional parameterlist. This
     list may be non-existent, that is no parentheses are specified at
     all, an empty list may be specified, that is empty parentheses are
     following the function name, or parameters are specified between
     the parentheses. A parameter is specified using a '$'-sign
     followed by a non-fractional number. The counting starts with one
     and has to be specified. A function using one parameter has to
     specify the parameter '$1'. If more than one parameter is used, the
     parameters have to be numbered consecutively. The symbol '<NUM>'
     in the syntax diagram below denotes the highest number of the
     parameters. The symbol '$#' denotes a variable number of
     arguments. Any number of arguments may be assigned to this symbol
     including zero. The last two lines of the syntax diagram below
     denote the way of specifying a number of arguments that have to be
     specified for this particular function.

          `$1

          $1, ..., $<NUM>

          $#

          $1, $#

          $1, ..., $<NUM>, $#
          '

     ADiMat compares the signature specified by these parameter list
     with the list of arguments the function is called in the program
     to differentiate with. If the numbers do not match, a warning is
     issued.  The return arguments are specified similiar. If only one
     single return argument is to be specified, this may be done using
     the '$$'-expression which is an abbreviation for '[$$1]'. If more
     than one return argument is to be specified, they have to be
     written in square brackets. The '$$#'-expression has to be
     specified in square bracktes, even if it is the only return
     argument. The '$$#'-expression denotes that a function returns
     more than one result similiar to the '$#'-expression in the
     parameter list.

`Examples:'
     These are valid function-signatures:

           `$$= g($1, $2:real)

          f1

          [$$1:string, $$2]=
          example($1:matrix)

          [$$#]=varargout_function($1)

          printf($1:string, $#)
          '



File: adimat.info,  Node: actionstring,  Next: types,  Prev: funcsig,  Up: Builtins

14.4 actionstring
=================

`Description:'
     Define what code to insert when differentiating the corresponding
     function.

`Syntax and semantic:'
     The code in a `actionstring' has to be valid Matlab-code.
     Expressions starting with a '$' are replaced. The following table
     gives the '$'-expressions and what is substituted for them.

          '$'-expression   |  substitute
          $<NUM>   |  Is substituted by the <NUM>'s argument of the
          function.
          $@<NUM>   |  Is substituted by the derivative of the argument
          $<NUM>.
          $$<NUM>   |  Is substituted by the <NUM>'s result of the
          original expression.
          $$@<NUM>   |  Is substituted by the identifier of the
          derivative of the <NUM>'s result (only applicable with *note
          BMFUNC:: , error else).
          $TMP<NUM>   |  Is substituted by a temporary variable that is
          unique for the difftostring of the current expression.
          Allthough not forced, this symbol make sense in *note BMFUNC::
          only.
          $@TMP<NUM>   |  Is substituted by the derivative of the
          <NUM>'s temporary variable. An association by name is done to
          the <NUM> temporary variable. It is recommended
          that derivatives of temporary variables are referred to by
          this symbol. Allthough not forced, this symbol make sense in
          *note BMFUNC::  only.
          $#   |  Is substituted by the arguments, that are associated
          to the varargin.
          $@#   |  Is substituted by the list of derivatives of the
          arguments, that are associated to the varargin. E.g.: `g_x,
          g_y' if `x, y' is bound to $#.
          $@m#   |  Is substituted by the list of derivatives followed
          by their original result. E.g.: `g_x, x, g_y, y' if `x, y' is
          bound to $#.
          $$#   |  Is substituted by the list of results, that are
          associated to the varargout.
          $$@#   |  Is substituted by the list of derivatives of the
          results, that are associated to the varargout. E.g. `g_r,
          g_y' if `r, y' is associated to $$#

     !!! `actionstring' is not really a string. No doublequotes are
     allowed around it. A doublequote within it will start a string,
     that has to end with another doublequote in the same difftostring.

`Examples:'
     Look at *note BMFUNC::  for some applications.



File: adimat.info,  Node: types,  Next: BVAR,  Prev: actionstring,  Up: Builtins

14.5 types
==========

`Description:'
     The names of all available types and their meaning.

`Syntax:'
           `'real'

          'complex'

          'scalar'

          'matrix'

          'generic'

          'string'

          'boolean'

          'function''

`Semantic:'
     These identifiers represent simple types. 'real' and 'complex'
     type a parameter as a scaler real or complex number. The type
     'scalar' is an alias for 'complex', because every real number also
     is a complex number. 'scalar' was introduced to let the user
     express his intention to specify one single number more precisely.
     'matrix' types a real or complex matrix. There is no distinction
     between these two kinds, not yet.  'generic' types everything that
     is not explicitly typed. A generic parameter accepts every kind of
     data. 'real', 'complex', 'scalar', 'matrix', 'boolean', 'function'
     and 'string' are all accepted by a generic-typed parameter.
     'string' is a number of characters in single-quotes.  A 'boolean'
     only knows the two boolean-values true and false.  A
     'function'-typed parameter accepts a functionreference (the
     @-operator applied on a functionname).

`Examples:'
     Look at *note funcsig::  for some applications.



File: adimat.info,  Node: BVAR,  Next: BMFUNC,  Prev: types,  Up: Builtins

14.6 BVAR
=========

`Description:'
     Declare a variable.

`Syntax:'
           `'BVAR' <varname>

          'BVAR' <varname>':'type'

`Semantic:'
     Declare a variable. If the type is omitted 'generic' is used. This
     directive is used seldom. It is used however to spefify some
     constants like 'pi' or 'NaN' to be introduce them to ADiMat in a
     quick way.

`Examples:'
           BVAR test
          BVAR name:string
          BVAR pi:real



File: adimat.info,  Node: BMFUNC,  Next: BCFUNC,  Prev: BVAR,  Up: Builtins

14.7 BMFUNC
===========

`Description:'
     Declare a builtin MatLab function and its behaviour on
     differentiation.

`Syntax:'
           `'BMFUNC' funcsig 'DIFFTO' actionstring

          'BMFUNC' funcsig 'DIFFSTO' actionstring

          'BMFUNC' funcsig 'REPLACE' actionstring

          'BMFUNC' funcsig 'REPL' actionstring

          'BMFUNC' funcsig 'IGNORE'

          'BMFUNC' funcsig 'NODIFF'

          'BMFUNC' funcsig 'ERROR' errormsg

          'BMFUNC' funcsig 'WARNING' warningmsg

          'BMFUNC' funcsig 'WARN' warningmsg

          'BMFUNC' funcsig 'SPEC' eol'

`Semantic:'
     The BMFUNC-directive declares a function specified by `funcsig' to
     be built into Matlab. (look at *note funcsig::

     for more information about specifying function-signatures).  A
     'BMFUNC'-directive has two parts:

       1.  the definition-part ('BMFUNC' funcsig)

       2.  the action-part ('DIFFTO' actionstring, IGNORE, ...)

     The first is used to define a function-signature and the second
     tells ADiMat what to do if the function defined by `funcsig' is to
     be differentiated.  The possible actions are:

    `'DIFFSTO'|'DIFFTO' actionstring'
          The Matlab-expression specified by the `actionstring' is
          inserted into the code where the derivative of the function
          specified by `funcsig' is needed. Look at *note actionstring::
          for the specification of an `actionstring'.

    `'REPLACE'|'REPL' actionstring'
          The call to the function specified by `funcsig' is deleted
          from the statement-list during differentiation and
          substituted by the `actionstring'. For correct program
          behaviour the `actionstring' has to contain a semantically
          identical expression. That is, the original results have to
          contain the same values as in the undifferentiated program.
          This action is used to make use of functions, that compute
          additional information needed for the differentiation, but
          that is not yet required by the code. A REPLACE action may
          consist of more than one line of code, where a single \ at
          the end of a line acts as a line-continuation marker. The
          REPLACE action is available since ADiMat version 0.5 and in
          the dbbuild tool only. I.e. it can not be used as a directive!

    `'IGNORE''
          The function declared by `funcsig' is not differentiated. It
          will occur in the differentiated code the same way like in
          the undifferentiated code (e.g. 'BMFUNC $$=real($1) IGNORE'
          corresponds to 'BMFUNC $$=real($1) DIFFTO real($1)').

    `'NODIFF''
          The function declared by funcsig is not differentiable. When
          trying to differentiated it, ADiMat will stop with an error.
          The function declared by funcsig may be safely used in
          code-fragment that are not differentiated.

    `'ERROR' errormsg'
          Similar behaviour like 'NODIFF', but 'errormsg' is printed
          instead. 'errormsg' is a doublequoted string that may contain
          some special tokens:

         `$n, $i'
               the function-name,

         `$f'
               the filename,

         `$p'
               the pathname,

         `$l'
               the linenumber where the function occured,

         `$c'
               the character where the function occured.


          These special-tokens are replaced at outputtime and may occur
          in any nummer and order in the errorstring. The errormessage
          is written using printf, which means that the control codes
          like '\n' and so on will act as usual.

    `'WARING'|'WARN' warningmsg'
          The message given by `warningmsg' is printed during
          augmentation time and inserted into the output source code as
          comment. A WARNING action may be added to every other action
          including itself to issue the message given. A WARNING
          acition is added to another action by specifying a
          builtin-declaration with exactly the same `funcsig' and
          giving the WARNING message as action *before* the
          builtin-declaration the WARNING message it to be added. It is
          *not* possible to generally add a WARNING message to every
          occurrence of a function by specifying a more general
          signature or the most general signature `[$$#]= somefunc($#)'.
          The `warningmsg' may contain the same $-tokens like the
          `errormsg'.

    `'SPEC' eol'
          Is ignored currently. The spec-action is for future
          enhancements of ADiMat. All builtin-declarations that use
          this action produce a warning and are treated as if 'IGNORE'
          is specified instead. The line is read until its end and the
          contents is discarded.


`Examples:'
          BMFUNC $$:real=sin($1:real) DIFFSTO cos($@1)
          BMFUNC $$:real=real($1:scalar) IGNORE
          BMFUNC $$=abs($1) NODIFF
          BMFUNC nonlinear($1) ERROR "Function: $n used in file $f line $l: \n Function: is nonlinear -> not differentiable."
          BMFUNC $$=max($1) REPLACE {$$1, $TMP1}= max($1);\
          if (isvector($1)) ;\
             $$@1= $@1($TMP1);\
          else \
             $TMP2= size($1);\
             $$@= $@1((0:($TMP2(2)-1))*$TMP2( 1)+$TMP1);\
          end



File: adimat.info,  Node: BCFUNC,  Next: BJFUNC,  Prev: BMFUNC,  Up: Builtins

14.8 BCFUNC
===========

`Description:'
     Declare a function that is implemented in C-code.

`Syntax:'
           `'BCFUNC' funcsig 'DIFFTO' actionstring

          'BCFUNC' funcsig 'DIFFSTO' actionstring

          'BCFUNC' funcsig 'IGNORE'

          'BCFUNC' funcsig 'NODIFF'

          'BCFUNC' funcsig 'ERROR' errormsg

          'BCFUNC' funcsig 'SPEC' eol'

`Semantic:'
     Internally this directive is handle the same like BMFUNC. Except
     that a warning message is displayed at time, when this
     builtin-declaration is read.  This directive exists for
     future-enhancment. There may be more support for C-code in future.
     With this version of ADiMat the user has to specify the
     differentiated expression for this function AND provide it. (ADiC
     is a tool to differentiate C-code. It is planned to automatically
     call ADiC whenever a differentiated C-function is needed).

`Examples:'
     (see *note BMFUNC:: )



File: adimat.info,  Node: BJFUNC,  Next: The dbbuild tool,  Prev: BCFUNC,  Up: Builtins

14.9 BJFUNC
===========

`Description:'
     Declare a function that is implemented in Java-code.

`Syntax:'
           `'BJFUNC' funcsig 'DIFFTO' actionstring

          'BJFUNC' funcsig 'DIFFSTO' actionstring

          'BJFUNC' funcsig 'IGNORE'

          'BJFUNC' funcsig 'NODIFF'

          'BJFUNC' funcsig 'ERROR' string

          'BJFUNC' funcsig 'SPEC' eol'

`Semantic:'
     Internally this directiv is handle the same as BMFUNC. Except that
     a warning message is displayed when the builtin-delcaration is
     read. This directive exists for future-enhancment. There may be
     more support for Java-code in future (if an ADiJava is existent
     then). Nowadays the user has to specify the differentiated
     expression for this function AND provide it.

`Examples:'
     (see *note BMFUNC:: )



File: adimat.info,  Node: The dbbuild tool,  Prev: BJFUNC,  Up: Builtins

14.10 The dbbuild tool
======================


File: adimat.info,  Node: ToDo,  Next: Questions and Answers,  Prev: Builtins,  Up: Top

15 ToDo
*******


File: adimat.info,  Node: Questions and Answers,  Next: Examples,  Prev: ToDo,  Up: Top

16 Questions and Answers
************************

`What is the order of derivative objects and results or parameters in a'
     function call?  With *admDiffFor* and *admDiffVFor* the order of
     derivate objects and results or parameters in a function is: The
     derivative object preceeds its corresponding active result or
     active parameter. For example: Let $[y,w]=f(a,b,c)$ be a function
     having the parameters $a,b,$ and $c$ and the results $y$ and $w$.
     Assume that the parameters $a$ and $c$ and the output $y$ are
     active. The correct call of this function is:
     $[g_y,y,w]=f(g_a,a,b,g_c,c)$.

     With *admDiffRev* derivative outputs (adjoints of input parameters)
     get prepended to the output parameter list and derivative inputs
     (adjoints of output parameters) get appended to the input parameter
     list.

`Is the derivative assignment put in front of the orginial expression?'
     *admDiffFor*: The derivative assignment is put in front of the
     original assignment usually. The exceptions to this rule are
     assignments with LHSs with the backslash operator or other builtins
     with BMFUNC DIFFSTO rules that use the original expression's result
     ($$). In this case the original expression is assigned to a
     temporary variable which the derivative expression can use, and
     after that the temporary expression is assigned to its original
     variable.

     *admDiffVFor*: Only when an active variable is assigned a constant
     (non-active) value is the corresponding `d_zeros' assignment placed
     after the original assignment, so the RHSs value can be used there.

`Which prefix do derivative objects get?'
     *admDiffFor*: Derivative objects are prefixed with 'g_' by
     default. This may be changed using the parameter *note List of
     toolchains:: .

     *admDiffVFor*: Derivative objects are prefixed with 'd_' by
     default. This may be changed using the parameter *note List of
     toolchains:: .

     *admDiffRev*: Derivative objects are prefixed with 'a_' by
     default. This may be changed using the parameter *note List of
     toolchains:: .

`Which prefix do differentiated functions get?'
     *admDiffFor*: A differentiated function gets the prefix 'g_' by
     default. This may be changed using the parameter option *note List
     of toolchains:: .

     *admDiffVFor*: Derivative functions are prefixed with 'd_' by
     default. This may be changed using the parameter *note List of
     toolchains:: .

     *admDiffRev*: Adjoint functions are prefixed with 'a_' by default.
     This may be changed using the parameter *note List of toolchains::
     . Recording (forward-sweep) functions are prefixed with 'rec_' by
     default, changeable using the parameter *note List of toolchains::
     . Returning (reverse-sweep) functions are prefixed with 'ret_' by
     default, changeable using the parameter *note List of toolchains::
     .

`How can I put each generated function into a different file'
     With *admDiffVFor* and *admDiffRev* all generated functions are
     placed into a single file. You can set the parameter `output-mode'
     to `split-all' to change that. Be careful when using this as this
     may in certain circumstances overwrite some of your existing files.

`How can I avoid having to use use-subsref and use-subsasgn'
     In some situations, admDiffVFor will only work if the parameters
     use-subsref and/or use-subsasgn are turned on. Since these options
     make the AD code much slower, it may be worth the effort to change
     the code in such a way that they are not necessary any more.

     One such situation that may cause trouble is when you have a row
     vector and in your code index into it using a single index, like
     this: v(i), or like this: v(1:n). Try to change these expressions
     to v(1,i) and v(1,1:n), resp.

`How can I achieve a better performance'
     There are several things you can try in order to achieve a better
     performance:

        * First of all: try to write vectorized code.

        * When you have a rather small function, then the high-level
          drivers such as admDiffFor and adnDiffRev may require more
          time for performing all the useful checks and tests than for
          the actual derivative computation. Most of these checks can
          be turned off, using the option `nochecks':   adopts =
          admOptions();   adopts.nochecks = true; This is a shortcut
          option with the same effect as setting the following options:
           adopts.forceTransform = -1;   adopts.checkDependencies =
          false;   adopts.checkResultSizes = false;   adopts.checknargs
          = false;   adopts.checkoptions = false; Not however that this
          will in particular turn off the check whether the
          differentiated files are up to date. So it may be wise to run
          a dummy call `without' option `nocheck' before turning it on.

        * When using admDiffFor, try the different derivative classes
          (see *note The derivative classes:: ) to see which one is the
          fastest for you.

        * When you compute a Jacobian, see if it is sparse, so you
          might be able to apply compression. In exceptional cases,
          compression with the reverse mode may be an idea, if the
          Jacobian can be better compressed along the rows. Try
          admColorSeed with both the pattern and the transposed pattern
          to see if it makes a difference. Certain Jacobian with
          peculiar sparsity patterns (e.g. array shape) can be
          efficiently computed by a combination of the FM and RM,
          though you would have to do that manually.

        * When the NDD is low, but > 1 (for example after applying
          compression), and you are using admDiffFor or admDiffRev, you
          may try whether `strip mining' is faster than vector mode.
          The reason is that vector mode needs the derivative classes,
          which are rather slow. The only amortize when there is a
          certain minimum NDD. There are two ways to do this: Either
          set the option scalarModeSwitch to some number higher that
          your NDD or you set the option derivClassName to
          'scalar_directderivs'.

        * When using admDiffRev, try if in your code you can turn off
          the use of wrapper functions that deal with scalar expansion
          and implicit reshapes.    adopts = admOptions();
          adopts.parameters.allowArrayGrowth = 0;
          adopts.parameters.adjointReductions = 'no';
          adopts.parameters.adjointReshapes = 'no';
          adopts.parameters.avoidVarargFunctions = 1;

`How can I clean up the files generated by ADiMat'
     You can use the function *admClean* (cf.  *note Interfaces to
     program transformation:: ).

`How can I check derivatives for correctness'
     You should always check the AD derivatives that come out of ADiMat
     for correctness, at least initially and then after major changes
     to your code. We suggest that you test all three AD drivers and
     also the FD and complex variable method drivers (see *note High
     level user interface:: ) to compute the desired derivative. Then
     compute the relative error between any two of the resulting
     Jacobians J1 and J2 like this:

          norm(J1 - J2) ./ norm(J1)

     The relative error between any of the AD and the complex variable
     method derivatives should be very small, while the relative error
     between one of these and the FD derivatives have about the size of
     `sqrt(eps)'. You can use the functions *admAllDiff* and
     *admAllDiffMatrix* for this. See also our *note Examples::  on this
     topic.

`I am behind a web proxy which blocks me from accessing the transformation server. What can I do?'
     The ADiMat transformation server has been configured to listen
     also on the port 10443 for SSL connections in addition to the
     standard port 443. In some cases using this port allows you to get
     around the proxy.

     You can tell adimat-client to use this alternative port using one
     of these methods:

        * On the command-line, pass the option
          `-server=https://adimat.sc.informatik.tu-darmstadt.de:10443/'
          to adimat-client.

        * Within Matlab, you can add this same option to the `flags'
          field of admOptions.

        * Or you can set the server URL using the field `server' of
          admOptions.

`How can I tell ADiMat to ignore unbound identifiers?'
     You can give the flag `-b' to adimat-client, which will cause it
     to not generate errors when it encounters an unbound identifier.
     Unbound identifiers are most likely toolbox functions that ADiMat
     does not know. These may also include functions from ADiMat's
     runtime environment.

`I want to send ADiMat generated code to the server again, but I'
     get errors ADiMat generates code which may contain many calls to
     various runtime functions. These have often not been added to the
     function data base yet. Hence you will get errors complaining
     about unbound identifiers when you try to tranform this code again.

     See the *note Questions and Answers::  on how to suppress these.



File: adimat.info,  Node: Examples,  Next: Known bugs and limitations,  Prev: Questions and Answers,  Up: Top

17 Examples
***********

In this section we reference some examples MATLAB script that we
created using the the *publish* feature.

   Reverse mode speed test (reverse_mode_speed.html)

   Checking derivatives for correctness (check_derivatives.html)

   Using Taylor objects (taylor_class.html)

   Forward-over-reverse mode with reverse mode source differentiation
and Taylor objects (forward_oo_over_reverse_st.html)

   Hybrid mode differentiation (hybrid_mode_demo.html)

   Adding missing derivatives of builtins to ADiMat
(add_missing_derivative.html)


File: adimat.info,  Node: Known bugs and limitations,  Next: References,  Prev: Examples,  Up: Top

18 Known bugs and limitations
*****************************

`Index operations with repeated indices (Rev)'
     When using repeated indices in an index expression, for example
     x([2 2 2]), the RM differentiation will return wrong results.

`Tilde ignored results (For, VFor, Rev)'
     Ignoring output arguments using tilde (example: [~, S, ~] =
     svd(A)) is not supported because our parser is not able to
     recognize it. Dummy variable names must be used instead: [dummy,
     S, dummy] = svd(A)

`Index assignment from a call RHS (For)'
     When an index expression x(i) is assigned and the RHS is a
     non-active function call f(...), admDiffFor will afterwards clear
     the entire variable g_x and not just g_x(i). Workaround: use a
     temporary variable, like this:

          tmp = f(...);
          x(i) = tmp;

`Although certain builtins are implemented, some partials are still missing (For, VFor, Rev)'
     For certain builtins, whos derivative have been added to ADiMat,
     some partials are still missing. This means that you cannot
     differentiate those builtins w.r.t. certain parameters. The partial
     derivatives w.r.t those parameters are treated as zero, and
     unfortunately in most of the cases there will not even be a
     warning or error message. The following list attempts to collect
     the cases were we think that a builtin is differentiable w.r.t a
     certain parameter but the AD modes of ADiMat cannot compute that
     partial:

        * `norm(., P)' norm cannot be differentiated w.r.t to the
          second parameter P (the parameter p of the p-Norm)

        * `interp1(X,.,.,METHOD,.)' interp1 cannot be differentiated
          w.r.t to the first parameter X (points were values are given)
          when METHOD is not 'linear'

        * `besselh(NU,.,.)', `besseli(NU,.)', `besselj(NU,.)',
          `besselk(NU,.)', `bessely(NU,.)' Bessel functions cannot be
          differentiated w.r.t. the first parameter NU (order)


`When LHS values are indexed, their reuse in derivative computations fails (For)'
     As stated in *note Questions and Answers::

     admDiffFor will in some instances reuse the LHS of an assignment in
     derivative computations, like this:

             z= a/ b;
             g_r(1: l, 1)= (b' \ (g_a' - g_b' * z' ))' ;

     However, when the LHS is an indexed expression the shape of the
     value may implicitly change during the assignment, so that reusing
     the LHS value does not work as expected:

             r(:)= a/ b;
             g_r(1: l, 1)= (b' \ (g_a' - g_b' * r(:)' ))' ;

     The workaround is to introduce a temporary variable, like this:

             tmp= a/ b;
             r(:)= tmp;



File: adimat.info,  Node: References,  Next: Copyright,  Prev: Known bugs and limitations,  Up: Top

19 References
*************

`ADiFor'
     C. Bischof, A. Carle, P. Khademi, and A. Mauer, *ADIFOR 2.0:
     Automatic Differentiation of Fortran 77 Programs*, IEEE
     Computational Science & Engineering, 3(3):18-32, 1996.

`Automake'
     D. MacKenzie and T. Tromey, *GNU Automake version 1.4-p1*, Free
     Software Foundation, Inc, 1996. automake home
     (http://www.gnu.org/software/automake/automake.html)

`Autoconf'
     D. MacKenzie and B. Elliston, *Autoconf version 2.50*, Free
     Software Foundation, Inc, 2001. autoconf home
     (http://www.gnu.org/software/autoconf/autoconf.html)

`Evaluating derivatives'
     A. Griewank, *Evaluating Derivatives: Principles and Techniques of
     Algorithmic Differentiation*, SIAM, 2000.

`History of AD'
     M. Iri, *History of Automatic Differentiation and Rounding Error
     Estimation*, in Automatic Differentiation of Algorithms: Theory,
     Implementation, and Application, A. Griewank and G. F. Corliss,
     eds., 3-16, SIAM, 1991.

`VCG'
     G. Sander and I. Lemke, *VCG -- Visualization of Compiler Graphs*,
     user documentation v.1.30, University of Saarland, 1995.  vcg
     overview (http://rw4.cs.uni-sb.de/users/sander/html/gsvcg1.html).



File: adimat.info,  Node: Copyright,  Prev: References,  Up: Top

20 Copyright
************

ADiMat Copyright 2001-2008 Andre Vehreschild,
vehreschild@sc.rwth-aachen.de, Institute for Scientific Computing,
Aachen University, Germany.

   ADiMat Copyright 2009-2013 Johannes Willkomm,
johannes.willkomm@sc.tu-darmstadt.de Institute for Scientific
Computing, TU Darmstadt, Germany.



Tag Table:
Node: Top131
Node: Introduction885
Node: Changes1410
Node: Installation1883
Node: How to obtain ADiMat2184
Node: Requirements2883
Node: Installation on Linux or Unix systems3735
Node: Installation on Windows5888
Node: Building and installation on MacOS7619
Node: Supported Matlab constructs9501
Node: Supported Matlab features11258
Node: Partially supported Matlab features12021
Node: Matlab features not supported at all12837
Node: Limitations of the reverse mode (admDiffRev) (in addition to the above)13515
Node: Limitations of the classless vector mode (admDiffVFor)16197
Node: Limitations of admDiffFor and admDiffRev17429
Node: Using ADiMat18167
Node: General description18884
Node: High level user interface20032
Node: Higher order derivatives24123
Node: Taylor-Coefficients24688
Node: Hessians25976
Node: Hessians using admHessian26266
Node: Shortcut Hessian drivers30516
Node: Hessians by recursive application of admDiff31344
Node: Taylor-Coefficients in Reverse Mode33637
Node: Source code transformation35113
Node: Source transformation from within Matlab35787
Node: Source transformation from the command line36517
Node: Running the transformed code37725
Node: Selecting a runtime environment38540
Node: Running forward mode functions41048
Node: Running reverse mode functions43522
Node: Applying AD to an example45880
Node: Example using the high-level interface47040
Node: Example doing the differentiation steps manually49584
Node: Example using the scalar mode manually51610
Node: Using sparsity exploitation or compression53995
Node: Using sparsity exploitation with the high-level interface55058
Node: Using sparsity exploitation manually56576
Node: Tips and recipes58106
Node: Differentiating functions with char int struct or cell parameters58745
Node: Differentiating code in multiple directories61174
Node: The runtime environment62452
Node: High level user interface functions63076
Node: Interfaces to program transformation67635
Node: Derivative and adjoint classes70112
Node: Stacks for the reverse mode71473
Node: Other helper functions for the reverse mode73675
Node: The derivative classes73976
Node: Structure of derivative objects80825
Node: Creation of derivative objects82438
Node: Accessing the derivative data86317
Node: Constructor functions `create---()'91547
Node: The options system of the Matlab-class97389
Node: Matlab's cellarrays/structures and the derivative class99876
Node: Usage103204
Node: Modes and Toolchains103455
Node: List of toolchains105677
Node: Transformation parameters108882
Node: List of transformation parameters110040
Node: Usage of classic adimat110513
Node: adimat Options112209
Node: Directives130560
Node: adimat Directives130927
Node: admproc Directives150851
Node: Environment variables152450
Node: Dependecy graphs and further information156411
Node: Calltree of functions156977
Node: Variable dependency graph158862
Node: Debugging information160253
Node: Outlining - Simplifying complex expressions161992
Node: Builtins165541
Node: -amb-files168030
Node: Builtin-declarations169916
Node: funcsig170735
Node: actionstring173640
Node: types176178
Node: BVAR177548
Node: BMFUNC178086
Node: BCFUNC183553
Node: BJFUNC184586
Node: The dbbuild tool185492
Node: ToDo185615
Node: Questions and Answers185723
Node: Examples195130
Node: Known bugs and limitations195803
Node: References198626
Node: Copyright199949

End Tag Table
