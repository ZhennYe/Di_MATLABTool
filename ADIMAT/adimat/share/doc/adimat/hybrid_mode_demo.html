
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>hybrid_mode_demo</title><meta name="generator" content="MATLAB 7.12"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2013-06-11"><meta name="DC.source" content="hybrid_mode_demo.m"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h2>Contents</h2><div><ul><li><a href="#1">Hybrid mode differentiation example</a></li><li><a href="#4">Running the function f</a></li><li><a href="#5">Differentiation of the function f</a></li><li><a href="#6">Splitting the function into two</a></li><li><a href="#14">Getting some real speed</a></li></ul></div><h2>Hybrid mode differentiation example<a name="1"></a></h2><p>In this example we will differentiate a scalar function in a hybrid way, using both FM and RM AD. Since this is not supported directly by ADiMat, we have to split the function into two parts manually and apply ADiMat to each, and then combine the results to get the complete derivative.</p><p>The function is scalar, based on the Dirichlet energy function, and given in f.m:</p><pre class="codeinput">type <span class="string">f</span>
</pre><pre class="codeoutput">
function E = f(v, ne)
  n = length(v);
  E = 0;
  for i = 1:n
    for j = 1:4
      E = E + norm(v(i)-v(ne(i,j))).^2;
    end
  end

</pre><p>First we create the input arguments. v is a vector and ne is an array describing neighbour relations. Each v(i) has four neighbours:</p><pre class="codeinput">n = 500;
v = rand(n, 1);
ne = zeros(n, 4);

<span class="keyword">for</span> i=1:4
  ne(:,i) = circshift((1:n)', -i.^2);
<span class="keyword">end</span>
</pre><p>Next, we choose our FM differentiation function. The reason is the following: The function f performs only scalar computations, and in this setting admDiffVFor excells. So in this example we use admDiffFor, as it is more comparable to what admDiffRev does. admDiffVFor is so much faster that we would have to significantly increase the problem size n to see a difference between compression and no compression. But the reader is invited to try this demo with admDiffVFor as well.</p><pre class="codeinput">ForDiff = @admDiffFor;
<span class="comment">% ForDiff = @admDiffVFor;</span>
</pre><h2>Running the function f<a name="4"></a></h2><p>We run the function first:</p><pre class="codeinput">tic
E = f(v, ne);
tF = toc;
fprintf(1, <span class="string">'f took %g s\n'</span>, tF);
</pre><pre class="codeoutput">f took 0.002572 s
</pre><h2>Differentiation of the function f<a name="5"></a></h2><p>Now we plainly differentiate the function in both FM and RM</p><pre class="codeinput">adopts = admOptions(<span class="string">'i'</span>, 1);
adopts.functionResults = {E};

[Jf, zf, tf] = ForDiff(@f, 1, v, ne, adopts);
fprintf(1, <span class="string">'%s took %g s (Factor: %g)\n'</span>, func2str(ForDiff), tf.evaluation, tf.evaluation ./ tF);

[Jr, zr, tr] = admDiffRev(@f, 1, v, ne, adopts);
fprintf(1, <span class="string">'admDiffRev took %g s (Factor: %g)\n'</span>, tr.evaluation, tr.evaluation ./ tF);

assert(norm(Jr - Jf) ./ norm(Jr) &lt; 1e-14);
</pre><pre class="codeoutput">admDiffFor took 131.234 s (Factor: 51024)
admDiffRev took 8.15145 s (Factor: 3169.31)
</pre><h2>Splitting the function into two<a name="6"></a></h2><p>Now we split the function f into two functions g and h such that h(g(.)) == f(.).</p><pre class="codeinput">type <span class="string">g</span>

type <span class="string">h</span>
</pre><pre class="codeoutput">
function t = g(v, ne)
  n = length(v);
  t = zeros(length(v), 4);
  for i = 1:n
    for j = 1:4
      t(i,j) = norm(v(i)-v(ne(i,j))).^2;
    end
  end


function E = h(t)
  E = sum(t(:));

</pre><p>The idea here is that g is going to have a sparse Jacobian, so we can apply sparsity exploitation (compression) techniques to its computation. For that we need the sparsity pattern, which we first compute from the full Jacobian of g. This may be dangerous in the real world, but this is just an example here.</p><pre class="codeinput">adopts1 = admOptions(<span class="string">'i'</span>, 1);
[Jg] = admDiffVFor(@g, 1, v, ne, adopts1);
adopts1.JPattern = Jg ~= 0;

spy(adopts1.JPattern)
title(<span class="string">'The non-zero pattern of the Jacobian of g'</span>);
</pre><img vspace="5" hspace="5" src="hybrid_mode_demo_01.png" alt=""> <p>Now we compute the derivative of g in FM with compression. We also use strip-mining (scalar mode) since the number of directional derivatives is so small that we can avoid the overhead of the derivative classes:</p><pre class="codeinput">adopts1.derivClassName = <span class="string">'scalar_directderivs'</span>;
[Jg, t, tf] = ForDiff(@g, @cpr, v, ne, adopts1);
fprintf(1, <span class="string">'%s of g took %g s\n'</span>, func2str(ForDiff), tf.evaluation);
</pre><pre class="codeoutput">Colored the pattern with 3 colors
Compressed seed matrix: 500x3
admDiffFor of g took 5.11895 s
</pre><p>And we compute the derivative of h in RM:</p><pre class="codeinput">adopts2 = admOptions();
adopts2.functionResults = {E};

[Jh, Er, tr] = admDiffRev(@h, 1, t, adopts2);
fprintf(1, <span class="string">'Rev of h took %g s\n'</span>, tr.evaluation);
</pre><pre class="codeoutput">Rev of h took 0.00647753 s
</pre><p>For the final result we have to multiply both derivatives together as mandated by the chain rule: <img src="hybrid_mode_demo_eq21553.png" alt="$\frac{{\rm d} h(g(x))}{{\rm d} x} = \frac{{\rm d} h}{{\rm d} g} \frac{{\rm d} g}{{\rm d} x}$"></p><pre class="codeinput">tic
grad2 = Jh * Jg;
tM = toc;
</pre><p>So let's see what the whole endeavour gives us in terms of performance:</p><pre class="codeinput">tt = tf.evaluation + tr.evaluation + tM;
fprintf(1, <span class="string">'Hybrid differentiation took %g s (Factor: %g)\n'</span>, tt, tt ./ tF);
</pre><pre class="codeoutput">Hybrid differentiation took 5.12548 s (Factor: 1992.8)
</pre><p>And also check that the results are correct:</p><pre class="codeinput">assert((E - Er) ./ E &lt; 1e-14);
assert(norm(grad2 - Jr) ./ norm(grad2) &lt; 1e-14);
</pre><p>As a result we can state that in this example computing three directional derivatives in FM in three individual runs (scalar mode) is faster than a single run of scalar RM.</p><h2>Getting some real speed<a name="14"></a></h2><p>Just for the sake of completeness, let's also try if we can really get this example to speed. The way forward is to use vector operations in order to cut down on the number of statements that have to be executed for each number that we compute. For example, instead of looping over the neighbours, we can compute the distance to all of them at once for each v(i). This change results in the function f2:</p><pre class="codeinput">type <span class="string">f2</span>
</pre><pre class="codeoutput">
function E = f2(v, ne)
  n = length(v);
  E = 0;
  for i = 1:n
    E = E + norm(v(i)-v(ne(i,:))).^2;
  end

</pre><p>Differentiating this function should perform better already, since the function itself is more Matlab-friendly:</p><pre class="codeinput">tic
E = f2(v, ne);
tF = toc;
fprintf(1, <span class="string">'f3 took %g s\n'</span>, tF);

[Jf2, zf2, tf] = ForDiff(@f2, 1, v, ne, adopts);
fprintf(1, <span class="string">'%s took %g s (Factor: %g)\n'</span>, func2str(ForDiff), tf.evaluation, tf.evaluation ./ tF);

[Jr2, zr2, tr] = admDiffRev(@f2, 1, v, ne, adopts);
fprintf(1, <span class="string">'admDiffRev took %g s (Factor: %g)\n'</span>, tr.evaluation, tr.evaluation ./ tF);

assert(norm(Jr2 - Jf) ./ norm(Jr) &lt; 1e-14);
assert(norm(Jf2 - Jf) ./ norm(Jr) &lt; 1e-14);
</pre><pre class="codeoutput">f3 took 0.002117 s
admDiffFor took 35.3272 s (Factor: 16687.4)
admDiffRev took 2.08399 s (Factor: 984.405)
</pre><p>The same idea can also be done the other way around, computing the distance of all v(i) to their first neighbour. In this way we are left with a for loop with just 4 iterations. This leads to function f3:</p><pre class="codeinput">type <span class="string">f3</span>
</pre><pre class="codeoutput">
function E = f3(v, ne)
  E = 0;
  for i=1:4
    E = E + norm(v - v(ne(:,i))).^2;
  end

</pre><p>Differentiating f3 should now be really fast. Note that in this last example admDiffRev will be faster than admDiffVFor even for small n.</p><pre class="codeinput">tic
E = f3(v, ne);
tF = toc;
fprintf(1, <span class="string">'f3 took %g s\n'</span>, tF);

[Jf3, zf3, tf] = ForDiff(@f3, 1, v, ne, adopts);
fprintf(1, <span class="string">'%s took %g s (Factor: %g)\n'</span>, func2str(ForDiff), tf.evaluation, tf.evaluation ./ tF);

[Jr3, zr3, tr] = admDiffRev(@f3, 1, v, ne, adopts);
fprintf(1, <span class="string">'admDiffRev took %g s (Factor: %g)\n'</span>, tr.evaluation, tr.evaluation ./ tF);

assert(norm(Jr3 - Jf) ./ norm(Jr) &lt; 1e-14);
assert(norm(Jf3 - Jf) ./ norm(Jr) &lt; 1e-14);
</pre><pre class="codeoutput">f3 took 0.001082 s
admDiffFor took 0.449334 s (Factor: 415.281)
admDiffRev took 0.0224199 s (Factor: 20.7208)
</pre><p>
<small>Published with ADiMat version</small>
</p><pre class="codeinput">adimat_version(2)
</pre><pre class="codeoutput">
ans =

ADiMat 0.5.9-3713:3723M

</pre><p class="footer"><br>
      Published with MATLAB&reg; 7.12<br></p></div><!--
##### SOURCE BEGIN #####
%% Hybrid mode differentiation example
% In this example we will differentiate a scalar function in a hybrid
% way, using both FM and RM AD. Since this is not supported directly
% by ADiMat, we have to split the function into two parts manually and
% apply ADiMat to each, and then combine the results to get the
% complete derivative.
%
% The function is scalar, based on the Dirichlet energy function, and
% given in f.m:

type f

%%
% First we create the input arguments. v is a vector and ne is an
% array describing neighbour relations. Each v(i) has four neighbours:

n = 500;
v = rand(n, 1);
ne = zeros(n, 4);

for i=1:4
  ne(:,i) = circshift((1:n)', -i.^2);
end

%%
% Next, we choose our FM differentiation function. The reason is the
% following: The function f performs only scalar computations, and in
% this setting admDiffVFor excells. So in this example we use
% admDiffFor, as it is more comparable to what admDiffRev
% does. admDiffVFor is so much faster that we would have to
% significantly increase the problem size n to see a difference
% between compression and no compression. But the reader is invited to
% try this demo with admDiffVFor as well.

ForDiff = @admDiffFor;
% ForDiff = @admDiffVFor;

%% Running the function f
% We run the function first:

tic
E = f(v, ne);
tF = toc;
fprintf(1, 'f took %g s\n', tF);

%% Differentiation of the function f
% Now we plainly differentiate the function in both FM and RM

adopts = admOptions('i', 1);
adopts.functionResults = {E};

[Jf, zf, tf] = ForDiff(@f, 1, v, ne, adopts);
fprintf(1, '%s took %g s (Factor: %g)\n', func2str(ForDiff), tf.evaluation, tf.evaluation ./ tF);

[Jr, zr, tr] = admDiffRev(@f, 1, v, ne, adopts);
fprintf(1, 'admDiffRev took %g s (Factor: %g)\n', tr.evaluation, tr.evaluation ./ tF);

assert(norm(Jr - Jf) ./ norm(Jr) < 1e-14);

%% Splitting the function into two
% Now we split the function f into two functions g and h such that
% h(g(.)) == f(.). 

type g

type h

%%
% The idea here is that g is going to have a sparse Jacobian, so we
% can apply sparsity exploitation (compression) techniques to its
% computation. For that we need the sparsity pattern, which we first
% compute from the full Jacobian of g. This may be dangerous in the
% real world, but this is just an example here.

adopts1 = admOptions('i', 1);
[Jg] = admDiffVFor(@g, 1, v, ne, adopts1);
adopts1.JPattern = Jg ~= 0;

spy(adopts1.JPattern)
title('The non-zero pattern of the Jacobian of g');

%%
% Now we compute the derivative of g in FM with compression. We also
% use strip-mining (scalar mode) since the number of directional
% derivatives is so small that we can avoid the overhead of the
% derivative classes:

adopts1.derivClassName = 'scalar_directderivs';
[Jg, t, tf] = ForDiff(@g, @cpr, v, ne, adopts1);
fprintf(1, '%s of g took %g s\n', func2str(ForDiff), tf.evaluation);

%%
% And we compute the derivative of h in RM:

adopts2 = admOptions();
adopts2.functionResults = {E};

[Jh, Er, tr] = admDiffRev(@h, 1, t, adopts2);
fprintf(1, 'Rev of h took %g s\n', tr.evaluation);

%%
% For the final result we have to multiply both derivatives
% together as mandated by the chain rule: $\frac{{\rm d}
% h(g(x))}{{\rm d} x} = \frac{{\rm d} h}{{\rm d} g} \frac{{\rm d} g}{{\rm d} x}$

tic
grad2 = Jh * Jg;
tM = toc;

%%
% So let's see what the whole endeavour gives us in terms of
% performance:
tt = tf.evaluation + tr.evaluation + tM;
fprintf(1, 'Hybrid differentiation took %g s (Factor: %g)\n', tt, tt ./ tF);

%%
% And also check that the results are correct:

assert((E - Er) ./ E < 1e-14);
assert(norm(grad2 - Jr) ./ norm(grad2) < 1e-14);

%%
% As a result we can state that in this example computing three
% directional derivatives in FM in three individual runs (scalar mode)
% is faster than a single run of scalar RM.


%% Getting some real speed
% Just for the sake of completeness, let's also try if we can really
% get this example to speed. The way forward is to use vector
% operations in order to cut down on the number of statements that
% have to be executed for each number that we compute. For example,
% instead of looping over the neighbours, we can compute the distance
% to all of them at once for each v(i). This change results in the
% function f2:

type f2

%%
% Differentiating this function should perform better already, since
% the function itself is more Matlab-friendly:

tic
E = f2(v, ne);
tF = toc;
fprintf(1, 'f3 took %g s\n', tF);

[Jf2, zf2, tf] = ForDiff(@f2, 1, v, ne, adopts);
fprintf(1, '%s took %g s (Factor: %g)\n', func2str(ForDiff), tf.evaluation, tf.evaluation ./ tF);

[Jr2, zr2, tr] = admDiffRev(@f2, 1, v, ne, adopts);
fprintf(1, 'admDiffRev took %g s (Factor: %g)\n', tr.evaluation, tr.evaluation ./ tF);

assert(norm(Jr2 - Jf) ./ norm(Jr) < 1e-14);
assert(norm(Jf2 - Jf) ./ norm(Jr) < 1e-14);

%%
% The same idea can also be done the other way around, computing the
% distance of all v(i) to their first neighbour. In this way we are
% left with a for loop with just 4 iterations. This leads to function
% f3:

type f3

%%
% Differentiating f3 should now be really fast. Note that in this last
% example admDiffRev will be faster than admDiffVFor even for small n.

tic
E = f3(v, ne);
tF = toc;
fprintf(1, 'f3 took %g s\n', tF);

[Jf3, zf3, tf] = ForDiff(@f3, 1, v, ne, adopts);
fprintf(1, '%s took %g s (Factor: %g)\n', func2str(ForDiff), tf.evaluation, tf.evaluation ./ tF);

[Jr3, zr3, tr] = admDiffRev(@f3, 1, v, ne, adopts);
fprintf(1, 'admDiffRev took %g s (Factor: %g)\n', tr.evaluation, tr.evaluation ./ tF);

assert(norm(Jr3 - Jf) ./ norm(Jr) < 1e-14);
assert(norm(Jf3 - Jf) ./ norm(Jr) < 1e-14);

%%
% <html>
% <small>Published with ADiMat version</small>
% </html>
adimat_version(2)

##### SOURCE END #####
--></body></html>